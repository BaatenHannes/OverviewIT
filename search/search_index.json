{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SOLID Single Responsibility Principle Elke klasse of module heeft de verantwoordelijkheid over \u00e9\u00e9n functionaliteit. (Kan doorgetrokken worden tot methodes). Open/Closed \u201cSoftware entities (classes, modules, functions, etc.) should be open for extension, but closed for modification\u201d. Je moet een klasse of module kunnen uitbreiden (open) zonder ze aan te passen (closed). Een klasse moet verschillende scenario\u2019s aankunnen die nog niet nader gedefinieerd zijn, zonder ze hiervoor aan te passen. Bvb: klasse rechthoek met lengte en breedte, en een klasse voor het berekenen van een totale oppervlakte van een verzameling van rechthoeken. Public class Rectangle { public double Width {get; set;} public double Height {get; set;} } Public class AreaCalculator { public double Area(Rectangle[] shapes) { double area = 0; foreach(var shape in shapes) { area += shape.Width*shape.Height; } return area; } } Bovenstaande klasse kan niet uitgebreid worden naar andere objecten, beter is dus de oppervlakte op object niveau, zodat de areacalculator kan uitgebreid worden naar andere objecten zoals cirkels, zonder deze te moeten aanpassen: public double Area(Shape[] shapes) { double area = 0; foreach (var shape in shapes) { area += shape.Area(); } return area; } Liskov Substitution Elke afgeleide klasse moet gesubstitueerd kunnen worden voor zijn base/parent. Wanneer een klasse dus in plaats van zijn parent gebruikt wordt, moet alle functionaliteit op dezelfde manier blijven werken. A great example illustrating LSP (given by Uncle Bob in a podcast I heard recently) was how sometimes something that sounds right in natural language doesn't quite work in code. In mathematics, a Square is a Rectangle. Indeed it is a specialization of a rectangle. The \"is a\" makes you want to model this with inheritance. However if in code you made Square derive from Rectangle, then a Square should be usable anywhere you expect a Rectangle. This makes for some strange behavior. Imagine you had SetWidth and SetHeight methods on your Rectangle base class; this seems perfectly logical. However if your Rectangle reference pointed to a Square, then SetWidth and SetHeight doesn't make sense because setting one would change the other to match it. In this case Square fails the Liskov Substitution Test with Rectangle and the abstraction of having Square inherit from Rectangle is a bad one. Interface Segregation Elke implementatie van een interface of supertype moet gebruik kunnen maken van alle methodes die overge\u00ebrfd of ge\u00efmplementeerd worden. Er zou nooit een nutteloze methode mogen ge\u00ebrfd of ge\u00efmplementeerd worden. Het opsplitsen van interfaces en klassen indien nodig. Hangt sterk samen met Single Responsibility Principle Verschil tussen Single Responsibility Principle en Interface Segregation is vanwaar je het bekijkt. Zelfde principe vanuit twee verschillende oogpunten: SRP tells us that you should only have a single responsibility in a module. ISP tells us that you should not be forced to be confronted with more than you actually need. If you want to use a print() method from interface I, you shouldn't have to instantiate a SwimmingPoolor a DriveThru class for that. More concretely, and going straight to the point, they are different views on the same idea -- SRP is more focused on the designer-side point-of-view, while ISP is more focused on the client-side point-of-view. So you're basically right. The ISP was first used and formulated by Robert C. Martin while consulting for Xerox. Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class. Dependency Inversion High level modules should not depend upon low level modules. Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions. Zorgt voor Decoupling - Loose coupling In volgend voorbeeld is de hogere level klasse Notification afhankelijk van een lager level klasse Email. Bij nieuwe objecten moet de code gewijzigd worden. public class Email { public void SendEmail() { // code } } public class Notification { private Email _email; public Notification() { _email = new Email(); } public void PromotionalNotification() { _email.SendEmail(); } } We lossen deze afhankelijkheid op door een abstractie te maken tussen de twee klassen. Nu is notificatie enkel afhankelijk van een abstracte messageservice, waardoor nieuwe implementaties eenvoudig toegevoegd kunnen worden. public interface IMessageService { void SendMessage(); } public class Email : IMessageService { public void SendMessage() { // code } } public class Notification { private IMessageService _iMessageService; public Notification() { _iMessageService = new Email(); } public void PromotionalNotification() { _iMessageService.SendMessage(); } } We moeten enkel nog via Dependency Injection de afhankelijkheid weghalen door het object via de constructor te injecteren zoals hieronder. public class Notification { private IMessageService _iMessageService; public Notification(IMessageService _messageService) { this._iMessageService = _messageService; } public void PromotionalNotification() { _iMessageService.SendMessage(); } } Dependency Inversion vs Dependency Injection Als we in onderstaand voorbeeld geen Interface injecteren via Dependency Injection, maar een concrete implementatie via een object, dan doen we aan Dependency Injection maar niet aan Dependency Inversion: de klasse geeft de controle van het instanti\u00ebren af, maar is afhankelijk van een concrete implementatie ipv een abstractie waardoor de klassen nog steeds niet loosely coupled zijn. public class Notification { private IMessageService _iMessageService; public Notification(IMessageService _messageService) { this._iMessageService = _messageService; } public void PromotionalNotification() { _iMessageService.SendMessage(); } } Inversion Of Control 'Omdraaien van de controle'. The division between the 'What' and the 'When' parts of the code. Code that does not know and care when it will run, it only knows what it will do and depend on abstractions. In traditional programming, the flow of the business logic is determined by objects that are statically bound to one another. With inversion of control, the flow depends on the object graph that is built up during program execution. Such a dynamic flow is made possible by object interactions that are defined through abstractions. This run-time binding is achieved by mechanisms such as dependency injection or a service locator. Factory pattern Service locator pattern Dpeendency injection Strategy pattern Patterns Factory method pattern In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created, by calling a factory method rather than by calling a constructor . Puts the pieces together to make a complete object and hides the concrete type from the caller. \"Define an interface for creating an object, but let subclasses decide which class to instantiate. The Factory method lets a class defer instantiation it uses to subclasses.\" (Gang Of Four) Why and when to use: * A class cannot anticipate the type of objects it needs to create beforehand. * A class requires its subclasses to specify the objects it creates. * You want to localize the logic to instantiate a complex object. Abstract factory pattern Variant on the factory method pattern, where instead of creating object in a factory method, you use a factory class to generate objects of a certain type. This is especially usefull when creating complex objects in multiple different classes. They can all share a factory class to generate objects, and if the composition of the object changes, only the factory class needs to be changed. the Factory Method pattern uses inheritance and relies on a subclass to handle the desired object instantiation. with the Abstract Factory pattern , a class delegates the responsibility of object instantiation to another object via composition. Factory method Creates object of a subclass of Foo. Which subclass depends on the subclass of A. Inheritence is used to decide the behaviour of f.whatever(). class A { public void doSomething() { Foo f = makeFoo(); f.whatever(); } protected Foo makeFoo() { return new RegularFoo(); } } class B extends A { protected Foo makeFoo() { //subclass is overriding the factory method //to return something different return new SpecialFoo(); } } Abstract Factory Creates objects of a subclass of Foo. Which subclass depends on the specifiek factory that is injected. This way, the behaviour of f.whatever() depends on the type of factory you inject. class A { private IFactory factory; public A(IFactory factory) { this.factory = factory; } public void doSomething() { //The concrete class of \"f\" depends on the concrete class //of the factory passed into the constructor. If you provide a //different factory, you get a different Foo object. Foo f = factory.makeFoo(); f.whatever(); } } interface IFactory { Foo makeFoo(); Bar makeBar(); Aycufcn makeAmbiguousYetCommonlyUsedFakeClassName(); } //need to make concrete factories that implement the \"Factory\" interface here Strategy Pattern The strategy pattern is a behavioral software design pattern that enables selecting an algorithm at runtime. Instead of implementing a single algorithm directly, code receives run-time instructions as to which in a family of algorithms to use. Strategy can be assigned to an object by composition, rather then inheritance, implementing an IStrategy interface where you call for example Strategy.DoSomething(). The type of IStrategy will decide the behaviour of the method, and the object can be switched at runtime with another strategy object with another implementation. public static void Main(String[] args) { // Prepare strategies IBillingStrategy normalStrategy = new NormalStrategy(); IBillingStrategy happyHourStrategy = new HappyHourStrategy(); Customer firstCustomer = new Customer(normalStrategy); // Normal billing firstCustomer.Add(1.0, 1); // Start Happy Hour firstCustomer.Strategy = happyHourStrategy; firstCustomer.Add(1.0, 2); // New Customer Customer secondCustomer = new Customer(happyHourStrategy); secondCustomer.Add(0.8, 1); // The Customer pays firstCustomer.PrintBill(); // End Happy Hour secondCustomer.Strategy = normalStrategy; secondCustomer.Add(1.3, 2); secondCustomer.Add(2.5, 1); secondCustomer.PrintBill(); } } The entire code of above example can be found here Singleton Pattern A singleton is a class which only allows one instance of itself to be created - and gives simple, easy access to said instance. This is achieved by: making the constructor private to prevent outside instantiation make a static property which contains one instance of itself public class Singleton { private Singleton() { // Prevent outside instantiation } private static readonly Singleton _singleton = new Singleton(); public static Singleton GetSingleton() { return _singleton; } } .NET .NET Framework The .NET Framework consists of the common language runtime (CLR) and the .NET Framework class library . De CLR (de runtime) is een virtuele machine en manages code at execution time: Geheugenbeheer Threadbeheer Exception handling Garbage collection Security Ontwikkelaars die CLR gebruiken schrijven hun code in een zogenaamd hogere programmeertaal zoals C#, Scala of VB.NET. Tijdens het compileren zorgt een .NET-compiler ervoor dat de broncode wordt omgezet naar MSIL, welke tijdens het uitvoeren door de just in time compiler van de CLR wordt gecompileerd naar code die uitgevoerd kan worden op het systeem waar de CLR op dat moment op draait. Dit zorgt ervoor dat applicaties niet hardware afhankelijk zijn, en dus op elk willekeurig systeem kunnen worden uitgevoerd, zolang er een CLR voor is. Door dit proces zijn applicaties wel (iets) trager, omdat ze tijdens het uitvoeren gecompileerd worden. REST Excellent overview here - https://restfulapi.net/statelessness/ Representational State Transfer is a software architecture used to create web services. RESTful web services allow the requesting systems to access and manipulate textual representations of web resources by using a uniform and predefined set of stateless operations. Architectural properties performance in component interactions, which can be the dominant factor in user-perceived performance and network efficiency;[9] scalability allowing the support of large numbers of components and interactions among components. simplicity of a uniform interface; modifiability of components to meet changing needs (even while the application is running); visibility of communication between components by service agents; portability - usability of the same software in different environments reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data.[9] Constraints CCCLUS - following these constraints ensures a RESTfull Webservice with the properties mentioned above. Client-server Architecture Separation of user interface and date storage Advantages: Portability: Multi platform Scalability: (only scale back-end for example) Statelessness No client context being stored on the server between requests Session state is held in the client Session state can be transferred by the server to another service such as a database to maintain a persistent state for a period and allow authentication Advantages: Scalability: any server can handle any request Simplicity Cacheability Responses must define themselves as cacheable or not to prevent clients from getting stale or inappropriate data in response to further requests. Eliminates some client\u2013server interactions. Scalability Performance Layered System Intermediary servers can improve system scalability by enabling load balancing and by providing shared caches. They can also enforce security policies. Scalability Reliability Code on demand Uniform interface Stateless Statelessness means that every HTTP request happens in complete isolation. When the client makes an HTTP request, it includes all information necessary for the server to fulfill that request. The server never relies on information from previous requests. If that information was important, the client would have sent it again in this request. Application State vs Resource State Application state is server-side data which servers store to identify incoming client requests, their previous interaction details, and current context information. Resource state is the current state of a resource on a server at any point of time \u2013 and it has nothing to do with the interaction between client and server. It is what you get as a response from the server as API response. You refer to it as resource representation. REST statelessness means being free on application state. Advantages of Statelessness There are some very noticeable advantages for having REST APIs stateless. Statelessness helps in scaling the APIs to millions of concurrent users by deploying it to multiple servers. Any server can handle any request because there is no session related dependency. Being stateless makes REST APIs less complex \u2013 by removing all server-side state synchronization logic. A stateless API is also easy to cache as well. A specific software can decide whether or not to cache the result of an HTTP request just by looking at that one request. There\u2019s no nagging uncertainty that state from a previous request might affect the cacheability of this one. It improves the performance of applications. The server never loses track of \u201cwhere\u201d each client is in the application because the client sends all necessary information with each request. Assembly vs Namespace vs Module An assembly provides a fundamental unit of physical code grouping. A namespace provides a fundamental unit of logical code grouping. Assembly Assembly is chunk of (precompiled) code that is physically grouped that can be executed by the .NET runtime environment. It contains one or more than one Namespaces . A .NET program consists of one or more assemblies. Assemblies are the fundamental building blocks. The code is grouped into a dll or exe. Namespace Namespace is a logical grouping of classes belongs to same functionality. First, the .NET Framework uses namespaces to organize its many classes Secondly, declaring your own namespaces can help control the scope of class and method names in larger programming projects. So System.Web and System.Data are namespaces. WCF Windows Communication Foundation, framework for building service-oriented applications. Can use multiple transport protocols (HTTP, TCP, IPC, ...). Commonly uses SOAP messages over Http. Message queuing. Yield Two advantages: * Custom iteration without creating temp collections * Stateful iteration Yield returns to the caller of the iteration, then goes back to the iteration. Normal iteration only stays in the iteration. Custom iteration Iterate without needing to create a temp collection. Easy for filtering existing lists. In example below, the yield return keeps iterating over the foreach, where a normal return would escape the function. int[] numbers = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }; IEnumerable<int> filteredNumbers = GetNumbers(numbers); //gets a enumerable with all numbers > 5 //without needing to create a temp collection private IEnumerable<int> GetNumbers(int[] nums) { foreach (int number in nums) { if (number > 5) { yield return number; } } } Stateful iteration When doing complex computations, yield can create the advantage that you can check for results before going to the next computation. A normal iteration needs to generate the full list first, then check for results. Yield gives the advantage of deferred execution, the list is only created dynamically while looping over the list. In example below, the check of i == 3 happens on each yield return, the yield return statements of 4 and 5 never happen. example 1 private static int GetItemFromList() { foreach (int i in GetList()) { if (i == 3) { return i; } } return 0; } private static IEnumerable<int> GetList() { yield return 1; yield return 2; yield return 3; yield return 4; yield return 5; } example 2 void ConsumeLoop() { foreach (Consumable item in ProduceList()) // might have to wait here item.Consume(); } IEnumerable<Consumable> ProduceList() { while (KeepProducing()) yield return ProduceExpensiveConsumable(); // expensive } Without yield, the call to ProduceList() might take a long time because you have to complete the list before returning: //pseudo-assembly Produce consumable[0] // expensive operation, e.g. disk I/O Produce consumable[1] // waiting... Produce consumable[2] // waiting... Produce consumable[3] // completed the consumable list Consume consumable[0] // start consuming Consume consumable[1] Consume consumable[2] Consume consumable[3]","title":"Theory"},{"location":"#solid","text":"","title":"SOLID"},{"location":"#single-responsibility-principle","text":"Elke klasse of module heeft de verantwoordelijkheid over \u00e9\u00e9n functionaliteit. (Kan doorgetrokken worden tot methodes).","title":"Single Responsibility Principle"},{"location":"#openclosed","text":"\u201cSoftware entities (classes, modules, functions, etc.) should be open for extension, but closed for modification\u201d. Je moet een klasse of module kunnen uitbreiden (open) zonder ze aan te passen (closed). Een klasse moet verschillende scenario\u2019s aankunnen die nog niet nader gedefinieerd zijn, zonder ze hiervoor aan te passen. Bvb: klasse rechthoek met lengte en breedte, en een klasse voor het berekenen van een totale oppervlakte van een verzameling van rechthoeken. Public class Rectangle { public double Width {get; set;} public double Height {get; set;} } Public class AreaCalculator { public double Area(Rectangle[] shapes) { double area = 0; foreach(var shape in shapes) { area += shape.Width*shape.Height; } return area; } } Bovenstaande klasse kan niet uitgebreid worden naar andere objecten, beter is dus de oppervlakte op object niveau, zodat de areacalculator kan uitgebreid worden naar andere objecten zoals cirkels, zonder deze te moeten aanpassen: public double Area(Shape[] shapes) { double area = 0; foreach (var shape in shapes) { area += shape.Area(); } return area; }","title":"Open/Closed"},{"location":"#liskov-substitution","text":"Elke afgeleide klasse moet gesubstitueerd kunnen worden voor zijn base/parent. Wanneer een klasse dus in plaats van zijn parent gebruikt wordt, moet alle functionaliteit op dezelfde manier blijven werken. A great example illustrating LSP (given by Uncle Bob in a podcast I heard recently) was how sometimes something that sounds right in natural language doesn't quite work in code. In mathematics, a Square is a Rectangle. Indeed it is a specialization of a rectangle. The \"is a\" makes you want to model this with inheritance. However if in code you made Square derive from Rectangle, then a Square should be usable anywhere you expect a Rectangle. This makes for some strange behavior. Imagine you had SetWidth and SetHeight methods on your Rectangle base class; this seems perfectly logical. However if your Rectangle reference pointed to a Square, then SetWidth and SetHeight doesn't make sense because setting one would change the other to match it. In this case Square fails the Liskov Substitution Test with Rectangle and the abstraction of having Square inherit from Rectangle is a bad one.","title":"Liskov Substitution"},{"location":"#interface-segregation","text":"Elke implementatie van een interface of supertype moet gebruik kunnen maken van alle methodes die overge\u00ebrfd of ge\u00efmplementeerd worden. Er zou nooit een nutteloze methode mogen ge\u00ebrfd of ge\u00efmplementeerd worden. Het opsplitsen van interfaces en klassen indien nodig. Hangt sterk samen met Single Responsibility Principle Verschil tussen Single Responsibility Principle en Interface Segregation is vanwaar je het bekijkt. Zelfde principe vanuit twee verschillende oogpunten: SRP tells us that you should only have a single responsibility in a module. ISP tells us that you should not be forced to be confronted with more than you actually need. If you want to use a print() method from interface I, you shouldn't have to instantiate a SwimmingPoolor a DriveThru class for that. More concretely, and going straight to the point, they are different views on the same idea -- SRP is more focused on the designer-side point-of-view, while ISP is more focused on the client-side point-of-view. So you're basically right. The ISP was first used and formulated by Robert C. Martin while consulting for Xerox. Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.","title":"Interface Segregation"},{"location":"#dependency-inversion","text":"High level modules should not depend upon low level modules. Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions. Zorgt voor Decoupling - Loose coupling In volgend voorbeeld is de hogere level klasse Notification afhankelijk van een lager level klasse Email. Bij nieuwe objecten moet de code gewijzigd worden. public class Email { public void SendEmail() { // code } } public class Notification { private Email _email; public Notification() { _email = new Email(); } public void PromotionalNotification() { _email.SendEmail(); } } We lossen deze afhankelijkheid op door een abstractie te maken tussen de twee klassen. Nu is notificatie enkel afhankelijk van een abstracte messageservice, waardoor nieuwe implementaties eenvoudig toegevoegd kunnen worden. public interface IMessageService { void SendMessage(); } public class Email : IMessageService { public void SendMessage() { // code } } public class Notification { private IMessageService _iMessageService; public Notification() { _iMessageService = new Email(); } public void PromotionalNotification() { _iMessageService.SendMessage(); } } We moeten enkel nog via Dependency Injection de afhankelijkheid weghalen door het object via de constructor te injecteren zoals hieronder. public class Notification { private IMessageService _iMessageService; public Notification(IMessageService _messageService) { this._iMessageService = _messageService; } public void PromotionalNotification() { _iMessageService.SendMessage(); } }","title":"Dependency Inversion"},{"location":"#dependency-inversion-vs-dependency-injection","text":"Als we in onderstaand voorbeeld geen Interface injecteren via Dependency Injection, maar een concrete implementatie via een object, dan doen we aan Dependency Injection maar niet aan Dependency Inversion: de klasse geeft de controle van het instanti\u00ebren af, maar is afhankelijk van een concrete implementatie ipv een abstractie waardoor de klassen nog steeds niet loosely coupled zijn. public class Notification { private IMessageService _iMessageService; public Notification(IMessageService _messageService) { this._iMessageService = _messageService; } public void PromotionalNotification() { _iMessageService.SendMessage(); } }","title":"Dependency Inversion vs Dependency Injection"},{"location":"#inversion-of-control","text":"'Omdraaien van de controle'. The division between the 'What' and the 'When' parts of the code. Code that does not know and care when it will run, it only knows what it will do and depend on abstractions. In traditional programming, the flow of the business logic is determined by objects that are statically bound to one another. With inversion of control, the flow depends on the object graph that is built up during program execution. Such a dynamic flow is made possible by object interactions that are defined through abstractions. This run-time binding is achieved by mechanisms such as dependency injection or a service locator. Factory pattern Service locator pattern Dpeendency injection Strategy pattern","title":"Inversion Of Control"},{"location":"#patterns","text":"","title":"Patterns"},{"location":"#factory-method-pattern","text":"In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created, by calling a factory method rather than by calling a constructor . Puts the pieces together to make a complete object and hides the concrete type from the caller. \"Define an interface for creating an object, but let subclasses decide which class to instantiate. The Factory method lets a class defer instantiation it uses to subclasses.\" (Gang Of Four) Why and when to use: * A class cannot anticipate the type of objects it needs to create beforehand. * A class requires its subclasses to specify the objects it creates. * You want to localize the logic to instantiate a complex object.","title":"Factory method pattern"},{"location":"#abstract-factory-pattern","text":"Variant on the factory method pattern, where instead of creating object in a factory method, you use a factory class to generate objects of a certain type. This is especially usefull when creating complex objects in multiple different classes. They can all share a factory class to generate objects, and if the composition of the object changes, only the factory class needs to be changed. the Factory Method pattern uses inheritance and relies on a subclass to handle the desired object instantiation. with the Abstract Factory pattern , a class delegates the responsibility of object instantiation to another object via composition. Factory method Creates object of a subclass of Foo. Which subclass depends on the subclass of A. Inheritence is used to decide the behaviour of f.whatever(). class A { public void doSomething() { Foo f = makeFoo(); f.whatever(); } protected Foo makeFoo() { return new RegularFoo(); } } class B extends A { protected Foo makeFoo() { //subclass is overriding the factory method //to return something different return new SpecialFoo(); } } Abstract Factory Creates objects of a subclass of Foo. Which subclass depends on the specifiek factory that is injected. This way, the behaviour of f.whatever() depends on the type of factory you inject. class A { private IFactory factory; public A(IFactory factory) { this.factory = factory; } public void doSomething() { //The concrete class of \"f\" depends on the concrete class //of the factory passed into the constructor. If you provide a //different factory, you get a different Foo object. Foo f = factory.makeFoo(); f.whatever(); } } interface IFactory { Foo makeFoo(); Bar makeBar(); Aycufcn makeAmbiguousYetCommonlyUsedFakeClassName(); } //need to make concrete factories that implement the \"Factory\" interface here","title":"Abstract factory pattern"},{"location":"#strategy-pattern","text":"The strategy pattern is a behavioral software design pattern that enables selecting an algorithm at runtime. Instead of implementing a single algorithm directly, code receives run-time instructions as to which in a family of algorithms to use. Strategy can be assigned to an object by composition, rather then inheritance, implementing an IStrategy interface where you call for example Strategy.DoSomething(). The type of IStrategy will decide the behaviour of the method, and the object can be switched at runtime with another strategy object with another implementation. public static void Main(String[] args) { // Prepare strategies IBillingStrategy normalStrategy = new NormalStrategy(); IBillingStrategy happyHourStrategy = new HappyHourStrategy(); Customer firstCustomer = new Customer(normalStrategy); // Normal billing firstCustomer.Add(1.0, 1); // Start Happy Hour firstCustomer.Strategy = happyHourStrategy; firstCustomer.Add(1.0, 2); // New Customer Customer secondCustomer = new Customer(happyHourStrategy); secondCustomer.Add(0.8, 1); // The Customer pays firstCustomer.PrintBill(); // End Happy Hour secondCustomer.Strategy = normalStrategy; secondCustomer.Add(1.3, 2); secondCustomer.Add(2.5, 1); secondCustomer.PrintBill(); } } The entire code of above example can be found here","title":"Strategy Pattern"},{"location":"#singleton-pattern","text":"A singleton is a class which only allows one instance of itself to be created - and gives simple, easy access to said instance. This is achieved by: making the constructor private to prevent outside instantiation make a static property which contains one instance of itself public class Singleton { private Singleton() { // Prevent outside instantiation } private static readonly Singleton _singleton = new Singleton(); public static Singleton GetSingleton() { return _singleton; } }","title":"Singleton Pattern"},{"location":"#net","text":"","title":".NET"},{"location":"#net-framework","text":"The .NET Framework consists of the common language runtime (CLR) and the .NET Framework class library . De CLR (de runtime) is een virtuele machine en manages code at execution time: Geheugenbeheer Threadbeheer Exception handling Garbage collection Security Ontwikkelaars die CLR gebruiken schrijven hun code in een zogenaamd hogere programmeertaal zoals C#, Scala of VB.NET. Tijdens het compileren zorgt een .NET-compiler ervoor dat de broncode wordt omgezet naar MSIL, welke tijdens het uitvoeren door de just in time compiler van de CLR wordt gecompileerd naar code die uitgevoerd kan worden op het systeem waar de CLR op dat moment op draait. Dit zorgt ervoor dat applicaties niet hardware afhankelijk zijn, en dus op elk willekeurig systeem kunnen worden uitgevoerd, zolang er een CLR voor is. Door dit proces zijn applicaties wel (iets) trager, omdat ze tijdens het uitvoeren gecompileerd worden.","title":".NET Framework"},{"location":"#rest","text":"Excellent overview here - https://restfulapi.net/statelessness/ Representational State Transfer is a software architecture used to create web services. RESTful web services allow the requesting systems to access and manipulate textual representations of web resources by using a uniform and predefined set of stateless operations.","title":"REST"},{"location":"#architectural-properties","text":"performance in component interactions, which can be the dominant factor in user-perceived performance and network efficiency;[9] scalability allowing the support of large numbers of components and interactions among components. simplicity of a uniform interface; modifiability of components to meet changing needs (even while the application is running); visibility of communication between components by service agents; portability - usability of the same software in different environments reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data.[9]","title":"Architectural properties"},{"location":"#constraints","text":"CCCLUS - following these constraints ensures a RESTfull Webservice with the properties mentioned above.","title":"Constraints"},{"location":"#client-server-architecture","text":"Separation of user interface and date storage Advantages: Portability: Multi platform Scalability: (only scale back-end for example)","title":"Client-server Architecture"},{"location":"#statelessness","text":"No client context being stored on the server between requests Session state is held in the client Session state can be transferred by the server to another service such as a database to maintain a persistent state for a period and allow authentication Advantages: Scalability: any server can handle any request Simplicity","title":"Statelessness"},{"location":"#cacheability","text":"Responses must define themselves as cacheable or not to prevent clients from getting stale or inappropriate data in response to further requests. Eliminates some client\u2013server interactions. Scalability Performance","title":"Cacheability"},{"location":"#layered-system","text":"Intermediary servers can improve system scalability by enabling load balancing and by providing shared caches. They can also enforce security policies. Scalability Reliability","title":"Layered System"},{"location":"#code-on-demand","text":"","title":"Code on demand"},{"location":"#uniform-interface","text":"","title":"Uniform interface"},{"location":"#stateless","text":"Statelessness means that every HTTP request happens in complete isolation. When the client makes an HTTP request, it includes all information necessary for the server to fulfill that request. The server never relies on information from previous requests. If that information was important, the client would have sent it again in this request.","title":"Stateless"},{"location":"#application-state-vs-resource-state","text":"Application state is server-side data which servers store to identify incoming client requests, their previous interaction details, and current context information. Resource state is the current state of a resource on a server at any point of time \u2013 and it has nothing to do with the interaction between client and server. It is what you get as a response from the server as API response. You refer to it as resource representation. REST statelessness means being free on application state.","title":"Application State vs Resource State"},{"location":"#advantages-of-statelessness","text":"There are some very noticeable advantages for having REST APIs stateless. Statelessness helps in scaling the APIs to millions of concurrent users by deploying it to multiple servers. Any server can handle any request because there is no session related dependency. Being stateless makes REST APIs less complex \u2013 by removing all server-side state synchronization logic. A stateless API is also easy to cache as well. A specific software can decide whether or not to cache the result of an HTTP request just by looking at that one request. There\u2019s no nagging uncertainty that state from a previous request might affect the cacheability of this one. It improves the performance of applications. The server never loses track of \u201cwhere\u201d each client is in the application because the client sends all necessary information with each request.","title":"Advantages of Statelessness"},{"location":"#assembly-vs-namespace-vs-module","text":"An assembly provides a fundamental unit of physical code grouping. A namespace provides a fundamental unit of logical code grouping.","title":"Assembly vs Namespace vs Module"},{"location":"#assembly","text":"Assembly is chunk of (precompiled) code that is physically grouped that can be executed by the .NET runtime environment. It contains one or more than one Namespaces . A .NET program consists of one or more assemblies. Assemblies are the fundamental building blocks. The code is grouped into a dll or exe.","title":"Assembly"},{"location":"#namespace","text":"Namespace is a logical grouping of classes belongs to same functionality. First, the .NET Framework uses namespaces to organize its many classes Secondly, declaring your own namespaces can help control the scope of class and method names in larger programming projects. So System.Web and System.Data are namespaces.","title":"Namespace"},{"location":"#wcf","text":"Windows Communication Foundation, framework for building service-oriented applications. Can use multiple transport protocols (HTTP, TCP, IPC, ...). Commonly uses SOAP messages over Http. Message queuing.","title":"WCF"},{"location":"#yield","text":"Two advantages: * Custom iteration without creating temp collections * Stateful iteration Yield returns to the caller of the iteration, then goes back to the iteration. Normal iteration only stays in the iteration.","title":"Yield"},{"location":"#custom-iteration","text":"Iterate without needing to create a temp collection. Easy for filtering existing lists. In example below, the yield return keeps iterating over the foreach, where a normal return would escape the function. int[] numbers = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }; IEnumerable<int> filteredNumbers = GetNumbers(numbers); //gets a enumerable with all numbers > 5 //without needing to create a temp collection private IEnumerable<int> GetNumbers(int[] nums) { foreach (int number in nums) { if (number > 5) { yield return number; } } }","title":"Custom iteration"},{"location":"#stateful-iteration","text":"When doing complex computations, yield can create the advantage that you can check for results before going to the next computation. A normal iteration needs to generate the full list first, then check for results. Yield gives the advantage of deferred execution, the list is only created dynamically while looping over the list. In example below, the check of i == 3 happens on each yield return, the yield return statements of 4 and 5 never happen. example 1 private static int GetItemFromList() { foreach (int i in GetList()) { if (i == 3) { return i; } } return 0; } private static IEnumerable<int> GetList() { yield return 1; yield return 2; yield return 3; yield return 4; yield return 5; } example 2 void ConsumeLoop() { foreach (Consumable item in ProduceList()) // might have to wait here item.Consume(); } IEnumerable<Consumable> ProduceList() { while (KeepProducing()) yield return ProduceExpensiveConsumable(); // expensive } Without yield, the call to ProduceList() might take a long time because you have to complete the list before returning: //pseudo-assembly Produce consumable[0] // expensive operation, e.g. disk I/O Produce consumable[1] // waiting... Produce consumable[2] // waiting... Produce consumable[3] // completed the consumable list Consume consumable[0] // start consuming Consume consumable[1] Consume consumable[2] Consume consumable[3]","title":"Stateful iteration"},{"location":"css/","text":"CSS Positioning elements Float float:left; The element needs to have specified width The element is removed from normal flow, but still part of the flow (unlike absolute positioning) It is shifted to the left, or right, until it touches the edge of its containing box, or another floated element. Other elements wrap around. To make sure elements go below the floated element, use clear: both Parent with only floated elements will collapse, fix by creating empty div behind parent element with clear: both or give parent element overflow: auto Flex-box Parent display: flex; --- defines the flex container flex-direction: row; --- defines the direction flex for child items justify-content: center; --- aligns child items on main axis align-items: flex-start; --- aligns child items on cross axis align-content: flex-start; --- aligns rows of items on cross axis (only when there are multipe rows of items) Child align-self: auto | center ...; --- allows default alignment to be overridden for this individual element order: <integer>; --- assigns the order of the item flex-grow: <integer>; --- defines the ability to take in more space, will overwrite the defined width (or height). The default is 0 (no grow, keep defined width). Value only has meaning relative to other flex-grow values, defines the proportion of space to take in relation to other flex objects. flex-shrink: <integer>; --- defines the ability to shrink if the container is too small. Default is 1, which means that every object will shrink equally. 0 will keep the object at its defined width. Higher values will make some object shrink faster than others. Nice link with gifs on flex-grow and flex-shrink Centering Horizontal Block element: margin: 0 auto or flexbox Text: text-align: center Vertical Block element: flexbox Text: line-height Perfect centering with flexbox Parent element: display: flex and define width and height Child element: margin: auto and define width and height Sizing elements Box-sizing With border-box, borders will grow towards the inside of the element instead of the outside. This ensures the total width of the element stays the same. Box-sizing: border-box; EM vs REM vs PX EM The em value is relative to the current font-size. If no font-size is declared in the element or parent element, the default font-size will be chosen which, in most browsers and user settings is 16px . In this instance, a value of 2em will mean 32px. Em is preferred over px because it is relative to the settings the user and browser have declared as standard. Px will override them, giving the user less options. REM REM takes the above value, but instead of being based on the current font-size of the element, it is based on the root font-size. For example: h1 { font-size: 2em; /* 1em = 16px */ margin-bottom: 1em; /* 1em = 32px */ } In the example above, the margin bottom will be 32px, because it is based on the font-size of the element h1, which is set to 2*16px. If margin-bottom is set to 1rem, it will be 16px because it is based on the root font-size.","title":"CSS"},{"location":"css/#css","text":"","title":"CSS"},{"location":"css/#positioning-elements","text":"","title":"Positioning elements"},{"location":"css/#float","text":"float:left; The element needs to have specified width The element is removed from normal flow, but still part of the flow (unlike absolute positioning) It is shifted to the left, or right, until it touches the edge of its containing box, or another floated element. Other elements wrap around. To make sure elements go below the floated element, use clear: both Parent with only floated elements will collapse, fix by creating empty div behind parent element with clear: both or give parent element overflow: auto","title":"Float"},{"location":"css/#flex-box","text":"","title":"Flex-box"},{"location":"css/#parent","text":"display: flex; --- defines the flex container flex-direction: row; --- defines the direction flex for child items justify-content: center; --- aligns child items on main axis align-items: flex-start; --- aligns child items on cross axis align-content: flex-start; --- aligns rows of items on cross axis (only when there are multipe rows of items)","title":"Parent"},{"location":"css/#child","text":"align-self: auto | center ...; --- allows default alignment to be overridden for this individual element order: <integer>; --- assigns the order of the item flex-grow: <integer>; --- defines the ability to take in more space, will overwrite the defined width (or height). The default is 0 (no grow, keep defined width). Value only has meaning relative to other flex-grow values, defines the proportion of space to take in relation to other flex objects. flex-shrink: <integer>; --- defines the ability to shrink if the container is too small. Default is 1, which means that every object will shrink equally. 0 will keep the object at its defined width. Higher values will make some object shrink faster than others. Nice link with gifs on flex-grow and flex-shrink","title":"Child"},{"location":"css/#centering","text":"","title":"Centering"},{"location":"css/#horizontal","text":"Block element: margin: 0 auto or flexbox Text: text-align: center","title":"Horizontal"},{"location":"css/#vertical","text":"Block element: flexbox Text: line-height","title":"Vertical"},{"location":"css/#perfect-centering-with-flexbox","text":"Parent element: display: flex and define width and height Child element: margin: auto and define width and height","title":"Perfect centering with flexbox"},{"location":"css/#sizing-elements","text":"","title":"Sizing elements"},{"location":"css/#box-sizing","text":"With border-box, borders will grow towards the inside of the element instead of the outside. This ensures the total width of the element stays the same. Box-sizing: border-box;","title":"Box-sizing"},{"location":"css/#em-vs-rem-vs-px","text":"","title":"EM vs REM vs PX"},{"location":"css/#em","text":"The em value is relative to the current font-size. If no font-size is declared in the element or parent element, the default font-size will be chosen which, in most browsers and user settings is 16px . In this instance, a value of 2em will mean 32px. Em is preferred over px because it is relative to the settings the user and browser have declared as standard. Px will override them, giving the user less options.","title":"EM"},{"location":"css/#rem","text":"REM takes the above value, but instead of being based on the current font-size of the element, it is based on the root font-size. For example: h1 { font-size: 2em; /* 1em = 16px */ margin-bottom: 1em; /* 1em = 32px */ } In the example above, the margin bottom will be 32px, because it is based on the font-size of the element h1, which is set to 2*16px. If margin-bottom is set to 1rem, it will be 16px because it is based on the root font-size.","title":"REM"},{"location":"git/","text":"GIT Overview Git is a version-control system for tracking changes in computer files and coordinating work on those files among multiple people. Example: stage, commit and push changes to remote repository called origin. git add * git commit -m \"bugfix\" git fetch origin git pull origin git push origin --all Example: push exisiting project to new remote repository (without readme) Step 1: Create local repository and commit changes git init git add . git commit -m \"Initial commit\" Step 2: add remote and push changes to remote git remote add origin remote_origin_url git remote -v (verifies then new remote URL) git push origin master Initialize git init --- create new local repository git clone --- create copy of an existing repository, automatically creates a remote connection called origin Saving changes The commands: git add , git status , and git commit are all used in combination to save a snapshot of a Git project's current state. git add --- stages changes git status --- checks which files are staged git reset --- undo a git add git commit --- commits the staged changes to the local repository Stashing changes Stashing takes your uncommitted changes (both staged and unstaged) , saves them away for later use, and then reverts them from your working copy. git stash --- temporarily shelves changes git stash pop --- reaplly stashed changes to local working copy Git stash does not stash untracked files (new files). To stash new files you can add -u (--include-untracked) git stash -u --- stashes changes with untracked files Syncing with remote repository Configuring remote A remote repository can be reached through a URL and credentials. Git remote allows you to creates an alias for easy use of the URL of the remote repository. git remote --- get current remote configurations git remote -v --- same as the above but includes the URL of each remote git remote add <name> <url> --- adds a new remote git remote rm <name> --- remove remote Pushing git push <remote> <branch> Merge vs Rebase Both try to achieve the same result, but rebase rewrites the history by 'rebasing' one branch on top of the other, creating a lineair history. This prevents cluttering with merge commits, but can create confusing situations by creating a different history with other team members. Link to explanation Git Reset If you made a local commit, but did not push it to remote yet, you can use a git reset to 'remove' the commit and keep or throw away the changes. -soft -- will keep the changes of the commit and stage them -mixed -- will keep the changes of the commit, but will not stage them -hard -- will throw away the changes of the commit","title":"GIT"},{"location":"git/#git","text":"","title":"GIT"},{"location":"git/#overview","text":"Git is a version-control system for tracking changes in computer files and coordinating work on those files among multiple people. Example: stage, commit and push changes to remote repository called origin. git add * git commit -m \"bugfix\" git fetch origin git pull origin git push origin --all Example: push exisiting project to new remote repository (without readme) Step 1: Create local repository and commit changes git init git add . git commit -m \"Initial commit\" Step 2: add remote and push changes to remote git remote add origin remote_origin_url git remote -v (verifies then new remote URL) git push origin master","title":"Overview"},{"location":"git/#initialize","text":"git init --- create new local repository git clone --- create copy of an existing repository, automatically creates a remote connection called origin","title":"Initialize"},{"location":"git/#saving-changes","text":"The commands: git add , git status , and git commit are all used in combination to save a snapshot of a Git project's current state. git add --- stages changes git status --- checks which files are staged git reset --- undo a git add git commit --- commits the staged changes to the local repository","title":"Saving changes"},{"location":"git/#stashing-changes","text":"Stashing takes your uncommitted changes (both staged and unstaged) , saves them away for later use, and then reverts them from your working copy. git stash --- temporarily shelves changes git stash pop --- reaplly stashed changes to local working copy Git stash does not stash untracked files (new files). To stash new files you can add -u (--include-untracked) git stash -u --- stashes changes with untracked files","title":"Stashing changes"},{"location":"git/#syncing-with-remote-repository","text":"","title":"Syncing with remote repository"},{"location":"git/#configuring-remote","text":"A remote repository can be reached through a URL and credentials. Git remote allows you to creates an alias for easy use of the URL of the remote repository. git remote --- get current remote configurations git remote -v --- same as the above but includes the URL of each remote git remote add <name> <url> --- adds a new remote git remote rm <name> --- remove remote","title":"Configuring remote"},{"location":"git/#pushing","text":"git push <remote> <branch>","title":"Pushing"},{"location":"git/#merge-vs-rebase","text":"Both try to achieve the same result, but rebase rewrites the history by 'rebasing' one branch on top of the other, creating a lineair history. This prevents cluttering with merge commits, but can create confusing situations by creating a different history with other team members. Link to explanation","title":"Merge vs Rebase"},{"location":"git/#git-reset","text":"If you made a local commit, but did not push it to remote yet, you can use a git reset to 'remove' the commit and keep or throw away the changes. -soft -- will keep the changes of the commit and stage them -mixed -- will keep the changes of the commit, but will not stage them -hard -- will throw away the changes of the commit","title":"Git Reset"},{"location":"linux/","text":"Linux Important Commands Overview ls - list cat - display contents of file (concatenate?) cd - change directory grep - search mv - move man - manual mkdir - make directory rmdir - remove directory touch - make empty file cp - copy rm - remove (file or directory) locate - find clear - clears the cli pwd - path to working directory (current location) nano / vi - creates or opens file in nano/vi text editor sudo - super user do tar - work with compression (tar ball archives) zip/unzip - work with comperssion (zip archives) chmod - change mode (change permissions, make file executable, etc.) hostname - displays hostname wc - word count (shows number of new lines, words and characters.) id - shows current user who - displays the list of current users who are logged in on the system w - same as who but more detailed + system status info Deep dive Piping Piping | command will send data from one program to another . Examples: ls | head -3 - will pipe the output of ls to the head command and only show the first three files Redirecting Redirecting >, >> or < commands will send data from a file to a program or from a program to a file . Examples: ls > myoutput - will create a file myoutput with the list of files/directories of current location ls >> myoutput - will add the output to the existing file wc < myoutput - will supply the content of the file myoutput to the command word count. Is more or less the same as the normal 'wc myoutput' as most commands will accept a file as input as an argument. LS ls -l - long listing (show details) ls -a - all (show hidden files) ls -lh - long listing human readable ls -R - recursively (show files inside directory tree) ls -lS - long listing with Size of files CAT cat filename.txt - show contents of filename cat filename.txt filename.txt - show contents of both files cat >filename.txt - create a file with filename, will await input of user. Close with ctrl+d cat filename | more - shows content but paginated with the more tool cat filename | less - shows content but paginated with the less tool cat -n filename.txt - shows content with line numbers cat filename.txt > filename2.txt - redirect output of filename.txt into new file filename2.txt cat filename.txt >> filename2.txt - redirect output of filename.txt into existing file filename2.txt cat filename | more - shows content but paginated with the less tool GREP grep 'word' filename - search file for word grep -i 'word' filename - case-insensitive search grep -R 'word' - searches all files in current directory and subdirectories for word grep -c 'word' filename - search and display number of times word is found Unix Principles Everything is a file Make each program do one thing well Basic scripting Hashbang: #! as first line of a text file creates an executable. #!/bin/bash -u The function of the hashbang is to tell the kernel what program to run as the script interpreter when the file is executed. Declaring a variable: VARIABLENAME=\"value\" Using a variable: echo $VARIABLENAME Read: prompts the user for input. Output of a script: 0 on succesfull, 1 on error. Exit 1 or Exit 0 will manually create output. '#': comment if: if command; then fi Package management Two large package managers: Debian (Debian distribution) RPM (Standard for other distributions - Red Hat) (Name from: Red Hat Package Manager) Debian package manager Extension: .deb Command: dpkg Command line front end programs: apt-get, aptitude GUI front-ends: synaptic, software-center Commands updated list: sudo apt-get update search by keyword in packages: sudo apt-cache search 'keyword' install or update: sudo apt-get install 'package' update all packages: sudo apt-get upgrade remove package: sudo apt-get remove 'package' (--purge to also delete config files) list all packages on your system: dpkg -l list all files of package: dpkg -L 'package' check if file is part of package: dpkg -S /path/to/file RPM Package Manager Extension: .rpm Command: rpm Command line front-end: yum, up2date (automatically resolve dependencies) GUI front-end: yumex, gpk-application Commands install: yum install 'package' search: yum search 'keyword' update: yum update 'package' update all packages: yum update remove: yum remove 'package' -- also removes all dependencies Process management BIOS BIOS (Basic Input/Output System) - software in non-volatile memory (firmware) on a chip on the system board BIOS loads Bootloader from harddisk to RAM - Bootloader loads the Kernel. Multi step process is needed to decouple the Operating System from the basic BIOS on the system. Kernel Is the Core of the Computer System. Is the connection between the application software and the hardware of the machine. Memory Management Resource Management Device Management System Calls Processes Information about the processes are available in pseudo-filesystems: Running processes are visible under /proc Hardware devices are available /dev Information about these devices /sys Other commands: pstree show the Process tree ps shows current running processes top shows an overview of all processes which keeps updating (like task manager) free shows memory management TTY Early user terminals connected to computers were electromechanical teleprinters or teletypewriters ( TeleTYpewriter , TTY), and since then TTY has continued to be used as the name for the text-only console although now this text-only console is a virtual console not a physical console. File system Linux seperates the static files from the dynamic files on different partitions. This prevents memory allocations for dynamic or user created files from interfering with the limited memory of the static file system. Root files: Windows: C:\\ Linux: / Linux 'hides' the location of the files. Windows shows this to the end user. In Windows, the structure of your directory and files are determined by the physical location of the files on the disk. In Linux, the location on the disk can be allocated to the file, and can be chosen or allocated by the sysadmin. Explanation File System Hierarchy Basic level Software /boot : Contains the Linux kernel and other bootloader software /bin : Contains binary executables to call (like commands ls, ...) /lib : System libraries (like System.dll in windows) (GAC?) Higher level software /opt : This directory is reserved for all the software and add-on packages that are not part of the default installation. Other /dev : is the location of special or device files. It is a very interesting directory that highlights one important aspect of the Linux filesystem - everything is a file or a directory. /dev/cdrom and /dev/fd0 represent your CD-ROM drive and your floppy drive. This may seem strange but it will make sense if you compare the characteristics of files to that of your hardware. Both can be read from and written to. Take /dev/dsp, for instance. This file represents your speaker device. Any data written to this file will be re-directed to your speaker. If you try 'cat /boot/vmlinuz > /dev/dsp' (on a properly configured system) you should hear some sound on the speaker. That's the sound of your kernel! A file sent to /dev/lp0 gets printed. Sending data to and reading from /dev/ttyS0 will allow you to communicate with a device attached there - for instance, your modem. Mounting Unix systems have a single directory tree. All accessible storage must have an associated location in this single directory tree. This is unlike Windows where (in the most common syntax for file paths) there is one directory tree per storage component (drive). Mounting is the act of associating a storage device to a particular location in the directory tree. For example, when the system boots, a particular storage device (commonly called the root partition) is associated with the root of the directory tree, i.e., that storage device is mounted on / (the root directory). Let's say you now want to access files on a CD-ROM. You must mount the CD-ROM on a location in the directory tree (this may be done automatically when you insert the CD). Let's say the CD-ROM device is /dev/cdrom and the chosen mount point is /media/cdrom. The corresponding command is mount /dev/cdrom /media/cdrom After that command is run, a file whose location on the CD-ROM is /dir/file is now accessible on your system as /media/cdrom/dir/file. When you've finished using the CD, you run the command umount /dev/cdrom or umount /media/cdrom (both will work; typical desktop environments will do this when you click on the \u201ceject\u201d or \u201dsafely remove\u201d button). Mounting applies to anything that is made accessible as files, not just actual storage devices. For example, all Linux systems have a special filesystem mounted under /proc. That filesystem (called proc) does not have underlying storage: the files in it give information about running processes and various other system information; the information is provided directly by the kernel from its in-memory data structures. System and User Security Users and groups Users are part of one or more groups, to be able to share data and files across the group. /etc : contains user and group data /etc/passwd : contains account info but no passwords /etc/shadow : contains the passwords of the users. ETC/PASSWD name:password placeholder:user id:primary group id:comment:home directory:shell Example: daemon:x:1:1:daemon:/usr/sbin:/bin/sh name (deamon): name of the account password placeholder (x): used to hold the password, is now replaced by a placeholder user id (1): the id of the user primary group id (1): the primary group of the user (every file has an owner and a group owner, which often is the primary group of the user) comment (deamon): can be used as a comment home directory (/user/sbin): the location of the users home directory (for normal users) shell (/bin/sh): the location of the users login shell, this is where the user is 'placed in' on login ETC/SHADOW name:password:last change:min:max:warn:inactive:expire:reserved Example: sysadmin:$6$lS6WJ9O/fNmEzrIi$kO9NKRBjLJJTlZD.L1Dc2xwcuUYaYwCTS.gt4elijSQW8ZDp6GLYAx.TRNNpUdAgUXUrzDuAPsYs5YHZNAorI1:15020:5:30:7:60:15050: name (sysadmin): name of the account password ($6...): the password encrypted (one-way) last change (15020): number of days since the last change (counting from 01-01-1970) min (5): Password aging field - after the user changes his password, he/she can't change it again after this amount of days (prevents changing the password immediately back to original) max (30): forces the user to change password after this amount of days warn (7): warns the user in x amount of days before password expiration inactive (60): grace period after which the user is locked out of the account (starts after the max) (if set to inactive, admin needs to reset password) expire (15050): sets the date when to expire the account (lock). Possible to reset the password by admin. System Accounts Root account is admin and has a id of 0. Normal accounts have an id greater then 500. System accounts : * id between 1 and 499 * designed to provide accounts for services that run on the system. * will have * instead of password Group membership /etc/passwd defines primary group membership /etc/group defines supplemental group memberships ETC/GROUP group_name:password_placeholder:GID:user_list Example: mail:x:12:mail,postfix user_list will display all members who are assigned to this group as a secondary membership. Primary membership is only defined in the /etc/passwd file. File access By default, any new file that a user creates will be owned by the user's primary group. -> this is probably the reason why a user will often have a group with the same name. Group ownership of a file can be changed by chgrp groupname filename Root User Root user has admin access. To change to root user, use su . In ubuntu, root account is disabled. Admin commands can then be run with sudo . To add users with admin priviledges, you need to edit the /etc/sudoers file with the command visudo as the root user. sysadmin ALL=(ALL) ALL will mean: the user sysadmin can on ALL machines act as ALL users to execute ALL commands. Creating Groups and Users Creating a Group groupadd groupname Reason: mostly for file sharing. Groupname considerations (to prevent issues): The first character: underscore _ or a lower-case letter a-z. Max 16 (can be up to 32 but may give issues on some distributions) remaining characters can be alphanumeric, the dash - or an underscore_. The last character should not be a hyphen -. example: hannes Modifying a group groupmod groupmod -n newname oldname -- change group name groupmod -g newid groupname -- change group id Changing the group name will have no impact on file access because the id is used as the identifier for the group. Changing the GID however will cause all associated files to become 'orphaned'. They will have a reference to a GID and no Group Name. Orphaned files can be found with find / -nogroup Deleting a group groupdel groupname This will also cause 'orphaned' files. Only supplemental groups can be deleted. Creating a User Command useradd Example useradd -u 1000 -g users -G wheel,research -c 'Jane Doe' jane Can be done manually, but is unsafe due to errors. In some distributions, creating a new users also creates a User Private Group (UPG) with the same name as the user. useradd will use some default settings to create new users. useradd -D view or change these default settings. (/etc/default/useradd file) Example output: GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes GROUP : default primary group (in distributions not using UPG's. Is normally the 'users' group) HOME : home directory INACTIVE : number of days after the password expires. (-1 means disabled) EXPIRE : expiration date SHELL : default shell SKEL : skeleton directory, can be used to populate default files and folder in the home directory of the user. CREATE_MAIL_SPOOL : local file where incoming email is placed. Other default values can be found in /etc/login.defs such as the mail directory, password max days, password min days, password min length, pasword warning age, uid min, uid max, gid min, gid max, umask etc. Advantages for multiple users selective access to certain files for certain users sudo command can be configured for selective commands and will log the usage by each user group membership for greater manamgement flexibility Changing a password passwd -> for own account passwd username -> for other account Changing an existing user account usermod username usermod -c comment username : comment usermod -d directory username : home directory usermod -f days username : inactive usermod -e expire_date username : expiration date usermod -g groupname username : set primary group usermod -G groupname,groupname username : set supplemental groups usermod -a groupname username : add supplemental groups usermod -l loginname username : set new login name usermod -L username : lock the user account usermod -s shell username : set the login shell usermod -u user_id username : set net userid usermod -U username : unlock the account Deleting a user userdel username : deletes the user but keeps the user's home directory userdel -r username : deleted user and home directory Ownership and permissions Each file has a user owner and a group owner. By default the creator of the file and the primary group of this account are the owners of the file. groups -- shows the groups of which the current user is part of newgrp groupname -- temporarily change the primary group to one of the supplemental groups (to create a file with different group permissions) chgrp groupname filename -- changes group ownership of an existing file. -R option for recursive change of directory stat filename -- shows detailed information about a file (group ownership etc.) chown username filename -- change user ownership chown username:groupname filename -- change user and group ownership chown :groupname filename -- change group ownership Permissions Source Viewing permissions Permissons are managed by rwx bits. -r read -w write -x execute Read: files can be read and copied. Directories can be listed without details. Write: Files can be written to and saved. Files can be added or remove from a directory. Read is necessary for correct usage. Execute: file can be executed or run. Directory can be changed by cd and directory can be used in path for commands. The permissions of all parent directories must be considered before considering the permissions on a specific file. If a user only has read access to a parent directory of a file, he cannot access the file. The w permission allows a user to delete files from a directory, but only if the user also has x permission on the directory. You can view these permissions by following command: ls -l filename.txt The permissions are managed for three different users/groups: u: User g: Group o: Others (all) Example output: -rw-rw-r-- 1 home home can be interpreted as read/write for home user, read/write for home group, read for all. Permissions are only valid for the contents of the file, not the file itself. Renaming a file for example looks at the permissions for the upper directory, instead of the file itself. Default permissions for files are not executable, but for directories are executable. For directories, this needs to be set to be able to call the ls command on the file. Changing permissions Chmod chmod options permissions file name Example symbolic mode: chmod u=rwx,g=rx,o=r myfile Example numeric mode: chmod 754 myfile Numeric mode: 4: \"read\" 2: \"write\" 1: \"execute\" 0: \"no permission.\" So 7 is the combination of permissions 4+2+1 (read, write, and execute), 5 is 4+0+1 (read, no write, and execute), and 4 is 4+0+0 (read, no write, and no execute). Symbolic mode: u: \"user\" g: \"group\" o: \"others\" a: \"all (user, group, other)\" +: add permission -: remove permission =: set exact permission r: read w: write x: execute s: setuid/setgid Example: add read permission for user on file abc.txt chmod u+r abc.txt chmod ug+r,o-w abc.txt -- add read for user owner and group owner, remove write from others Umask Umask (user file-creation mode mask) is a default set of permissions for a user when he/she created a file or new directory. The umask is defined by three numbers, and is subtracted from the default settings of 777 for directories and 666 for files. A umask of 002 would create directories with permission of 775 and files of 664. A high umask means a safer environment. umask -- shows the umask of the current user, ex. 0002 The first number indicates that the umask is given as an octal number. Permanently changing a user's umask requires modifying the .bashrc file located in that user's home directory. File type ls -l /etc/passwd -rw-r--r--. 1 root root 4135 May 27 21:08 /etc/passwd First character points to the file type: * - :regular file * d :directory file * l :link (pointer to other file) * b :block file - relates to block hardware device where data is read in blocks * c :character file - read one byte at a time * p :pipe file * s :socker file - allows two processes o communicate Special permissions, Links and File locations SETUID permission (s or 4) Setuid allows a binary file to be run as the owner of the file instead of the user who executes the file. Some system utilities use this setting to allow a user to run a file with root priviledges. chmod u+s filename -- adds the setuid permission chmod 4*** filename -- adds the setuid numerically (replace * by the numeric permissions the file already has) SETGID permission Similar as the setuid permission, but for groups. Behavior depends on if the permission is set to a file or a directory. setgid on a file will allow a user to run an executable binary as temp member of the group. Example: /usr/bin/wall as executable with permissions: -rwxr-sr-x. 1 root tty 10996 Jul 19 2011 /usr/bin/wall The s permission on the group allows the user to execute this executable as member of the tty group and thus also access all necessery files to execute this command. setgid on a directory causes files that are created in the directory to be owner by the group that owns the directory. Normally they are owned by the primary group of the user. The newly created directories will also have the same setgid permission. This is necessary for team work if users with different primary groups need to create new files and directories in a shared directory. Sticky Bit permission (t or 1) Prevents other users from deleting your files in a shared directory. Only the owner will be able to delete the file. Symbolic: chmod o+t directoryname Numeric: chmod 1775 filename/directoryname (add 1 before the existing permission) lowercase t - both sticky bit and execution are set for 'others' uppercate T - only sticky bit is set for 'others' Links Hard to access files (with long url's, deeply buried in directories) can be 'copied' to a link file. This file acts as a shorcut to the file. ls -li filename - shows number of existing links for a file ln existingfilename newfilename - creates a hard link ln -s existingfilename newfilename - creates a soft link Hard link will create file which points to same inode. Soft link will point to filename. Deleting 'hard link' file will only work if all files which point to the same inode are deleted. -> safer Soft link works with directories. Soft link can work between different file systems (partitions). Difference between hard and soft links: Underneath the file system files are represented by inodes. A file in the file system is basically a link to an inode. A hard link then just creates another file with a link to the same underlying inode. When you delete a file it removes one link to the underlying inode. The inode is only deleted (or deletable/over-writable) when all links to the inode have been deleted. A symbolic link is a link to another name in the file system. Once a hard link has been made the link is to the inode. deleting renaming or moving the original file will not affect the hard link as it links to the underlying inode. Any changes to the data on the inode is reflected in all files that refer to that inode. Note: Hard links are only valid within the same File System. Symbolic links can span file systems as they are simply the name of another file.","title":"Linux"},{"location":"linux/#linux","text":"","title":"Linux"},{"location":"linux/#important-commands","text":"","title":"Important Commands"},{"location":"linux/#overview","text":"ls - list cat - display contents of file (concatenate?) cd - change directory grep - search mv - move man - manual mkdir - make directory rmdir - remove directory touch - make empty file cp - copy rm - remove (file or directory) locate - find clear - clears the cli pwd - path to working directory (current location) nano / vi - creates or opens file in nano/vi text editor sudo - super user do tar - work with compression (tar ball archives) zip/unzip - work with comperssion (zip archives) chmod - change mode (change permissions, make file executable, etc.) hostname - displays hostname wc - word count (shows number of new lines, words and characters.) id - shows current user who - displays the list of current users who are logged in on the system w - same as who but more detailed + system status info","title":"Overview"},{"location":"linux/#deep-dive","text":"","title":"Deep dive"},{"location":"linux/#piping","text":"Piping | command will send data from one program to another . Examples: ls | head -3 - will pipe the output of ls to the head command and only show the first three files","title":"Piping"},{"location":"linux/#redirecting","text":"Redirecting >, >> or < commands will send data from a file to a program or from a program to a file . Examples: ls > myoutput - will create a file myoutput with the list of files/directories of current location ls >> myoutput - will add the output to the existing file wc < myoutput - will supply the content of the file myoutput to the command word count. Is more or less the same as the normal 'wc myoutput' as most commands will accept a file as input as an argument.","title":"Redirecting"},{"location":"linux/#ls","text":"ls -l - long listing (show details) ls -a - all (show hidden files) ls -lh - long listing human readable ls -R - recursively (show files inside directory tree) ls -lS - long listing with Size of files","title":"LS"},{"location":"linux/#cat","text":"cat filename.txt - show contents of filename cat filename.txt filename.txt - show contents of both files cat >filename.txt - create a file with filename, will await input of user. Close with ctrl+d cat filename | more - shows content but paginated with the more tool cat filename | less - shows content but paginated with the less tool cat -n filename.txt - shows content with line numbers cat filename.txt > filename2.txt - redirect output of filename.txt into new file filename2.txt cat filename.txt >> filename2.txt - redirect output of filename.txt into existing file filename2.txt cat filename | more - shows content but paginated with the less tool","title":"CAT"},{"location":"linux/#grep","text":"grep 'word' filename - search file for word grep -i 'word' filename - case-insensitive search grep -R 'word' - searches all files in current directory and subdirectories for word grep -c 'word' filename - search and display number of times word is found","title":"GREP"},{"location":"linux/#unix-principles","text":"Everything is a file Make each program do one thing well","title":"Unix Principles"},{"location":"linux/#basic-scripting","text":"Hashbang: #! as first line of a text file creates an executable. #!/bin/bash -u The function of the hashbang is to tell the kernel what program to run as the script interpreter when the file is executed. Declaring a variable: VARIABLENAME=\"value\" Using a variable: echo $VARIABLENAME Read: prompts the user for input. Output of a script: 0 on succesfull, 1 on error. Exit 1 or Exit 0 will manually create output. '#': comment if: if command; then fi","title":"Basic scripting"},{"location":"linux/#package-management","text":"Two large package managers: Debian (Debian distribution) RPM (Standard for other distributions - Red Hat) (Name from: Red Hat Package Manager)","title":"Package management"},{"location":"linux/#debian-package-manager","text":"Extension: .deb Command: dpkg Command line front end programs: apt-get, aptitude GUI front-ends: synaptic, software-center","title":"Debian package manager"},{"location":"linux/#commands","text":"updated list: sudo apt-get update search by keyword in packages: sudo apt-cache search 'keyword' install or update: sudo apt-get install 'package' update all packages: sudo apt-get upgrade remove package: sudo apt-get remove 'package' (--purge to also delete config files) list all packages on your system: dpkg -l list all files of package: dpkg -L 'package' check if file is part of package: dpkg -S /path/to/file","title":"Commands"},{"location":"linux/#rpm-package-manager","text":"Extension: .rpm Command: rpm Command line front-end: yum, up2date (automatically resolve dependencies) GUI front-end: yumex, gpk-application","title":"RPM Package Manager"},{"location":"linux/#commands_1","text":"install: yum install 'package' search: yum search 'keyword' update: yum update 'package' update all packages: yum update remove: yum remove 'package' -- also removes all dependencies","title":"Commands"},{"location":"linux/#process-management","text":"","title":"Process management"},{"location":"linux/#bios","text":"BIOS (Basic Input/Output System) - software in non-volatile memory (firmware) on a chip on the system board BIOS loads Bootloader from harddisk to RAM - Bootloader loads the Kernel. Multi step process is needed to decouple the Operating System from the basic BIOS on the system.","title":"BIOS"},{"location":"linux/#kernel","text":"Is the Core of the Computer System. Is the connection between the application software and the hardware of the machine. Memory Management Resource Management Device Management System Calls","title":"Kernel"},{"location":"linux/#processes","text":"Information about the processes are available in pseudo-filesystems: Running processes are visible under /proc Hardware devices are available /dev Information about these devices /sys Other commands: pstree show the Process tree ps shows current running processes top shows an overview of all processes which keeps updating (like task manager) free shows memory management","title":"Processes"},{"location":"linux/#tty","text":"Early user terminals connected to computers were electromechanical teleprinters or teletypewriters ( TeleTYpewriter , TTY), and since then TTY has continued to be used as the name for the text-only console although now this text-only console is a virtual console not a physical console.","title":"TTY"},{"location":"linux/#file-system","text":"Linux seperates the static files from the dynamic files on different partitions. This prevents memory allocations for dynamic or user created files from interfering with the limited memory of the static file system. Root files: Windows: C:\\ Linux: / Linux 'hides' the location of the files. Windows shows this to the end user. In Windows, the structure of your directory and files are determined by the physical location of the files on the disk. In Linux, the location on the disk can be allocated to the file, and can be chosen or allocated by the sysadmin. Explanation File System Hierarchy","title":"File system"},{"location":"linux/#basic-level-software","text":"/boot : Contains the Linux kernel and other bootloader software /bin : Contains binary executables to call (like commands ls, ...) /lib : System libraries (like System.dll in windows) (GAC?)","title":"Basic level Software"},{"location":"linux/#higher-level-software","text":"/opt : This directory is reserved for all the software and add-on packages that are not part of the default installation.","title":"Higher level software"},{"location":"linux/#other","text":"/dev : is the location of special or device files. It is a very interesting directory that highlights one important aspect of the Linux filesystem - everything is a file or a directory. /dev/cdrom and /dev/fd0 represent your CD-ROM drive and your floppy drive. This may seem strange but it will make sense if you compare the characteristics of files to that of your hardware. Both can be read from and written to. Take /dev/dsp, for instance. This file represents your speaker device. Any data written to this file will be re-directed to your speaker. If you try 'cat /boot/vmlinuz > /dev/dsp' (on a properly configured system) you should hear some sound on the speaker. That's the sound of your kernel! A file sent to /dev/lp0 gets printed. Sending data to and reading from /dev/ttyS0 will allow you to communicate with a device attached there - for instance, your modem.","title":"Other"},{"location":"linux/#mounting","text":"Unix systems have a single directory tree. All accessible storage must have an associated location in this single directory tree. This is unlike Windows where (in the most common syntax for file paths) there is one directory tree per storage component (drive). Mounting is the act of associating a storage device to a particular location in the directory tree. For example, when the system boots, a particular storage device (commonly called the root partition) is associated with the root of the directory tree, i.e., that storage device is mounted on / (the root directory). Let's say you now want to access files on a CD-ROM. You must mount the CD-ROM on a location in the directory tree (this may be done automatically when you insert the CD). Let's say the CD-ROM device is /dev/cdrom and the chosen mount point is /media/cdrom. The corresponding command is mount /dev/cdrom /media/cdrom After that command is run, a file whose location on the CD-ROM is /dir/file is now accessible on your system as /media/cdrom/dir/file. When you've finished using the CD, you run the command umount /dev/cdrom or umount /media/cdrom (both will work; typical desktop environments will do this when you click on the \u201ceject\u201d or \u201dsafely remove\u201d button). Mounting applies to anything that is made accessible as files, not just actual storage devices. For example, all Linux systems have a special filesystem mounted under /proc. That filesystem (called proc) does not have underlying storage: the files in it give information about running processes and various other system information; the information is provided directly by the kernel from its in-memory data structures.","title":"Mounting"},{"location":"linux/#system-and-user-security","text":"","title":"System and User Security"},{"location":"linux/#users-and-groups","text":"Users are part of one or more groups, to be able to share data and files across the group. /etc : contains user and group data /etc/passwd : contains account info but no passwords /etc/shadow : contains the passwords of the users.","title":"Users and groups"},{"location":"linux/#etcpasswd","text":"name:password placeholder:user id:primary group id:comment:home directory:shell Example: daemon:x:1:1:daemon:/usr/sbin:/bin/sh name (deamon): name of the account password placeholder (x): used to hold the password, is now replaced by a placeholder user id (1): the id of the user primary group id (1): the primary group of the user (every file has an owner and a group owner, which often is the primary group of the user) comment (deamon): can be used as a comment home directory (/user/sbin): the location of the users home directory (for normal users) shell (/bin/sh): the location of the users login shell, this is where the user is 'placed in' on login","title":"ETC/PASSWD"},{"location":"linux/#etcshadow","text":"name:password:last change:min:max:warn:inactive:expire:reserved Example: sysadmin:$6$lS6WJ9O/fNmEzrIi$kO9NKRBjLJJTlZD.L1Dc2xwcuUYaYwCTS.gt4elijSQW8ZDp6GLYAx.TRNNpUdAgUXUrzDuAPsYs5YHZNAorI1:15020:5:30:7:60:15050: name (sysadmin): name of the account password ($6...): the password encrypted (one-way) last change (15020): number of days since the last change (counting from 01-01-1970) min (5): Password aging field - after the user changes his password, he/she can't change it again after this amount of days (prevents changing the password immediately back to original) max (30): forces the user to change password after this amount of days warn (7): warns the user in x amount of days before password expiration inactive (60): grace period after which the user is locked out of the account (starts after the max) (if set to inactive, admin needs to reset password) expire (15050): sets the date when to expire the account (lock). Possible to reset the password by admin.","title":"ETC/SHADOW"},{"location":"linux/#system-accounts","text":"Root account is admin and has a id of 0. Normal accounts have an id greater then 500. System accounts : * id between 1 and 499 * designed to provide accounts for services that run on the system. * will have * instead of password","title":"System Accounts"},{"location":"linux/#group-membership","text":"/etc/passwd defines primary group membership /etc/group defines supplemental group memberships","title":"Group membership"},{"location":"linux/#etcgroup","text":"group_name:password_placeholder:GID:user_list Example: mail:x:12:mail,postfix user_list will display all members who are assigned to this group as a secondary membership. Primary membership is only defined in the /etc/passwd file.","title":"ETC/GROUP"},{"location":"linux/#file-access","text":"By default, any new file that a user creates will be owned by the user's primary group. -> this is probably the reason why a user will often have a group with the same name. Group ownership of a file can be changed by chgrp groupname filename","title":"File access"},{"location":"linux/#root-user","text":"Root user has admin access. To change to root user, use su . In ubuntu, root account is disabled. Admin commands can then be run with sudo . To add users with admin priviledges, you need to edit the /etc/sudoers file with the command visudo as the root user. sysadmin ALL=(ALL) ALL will mean: the user sysadmin can on ALL machines act as ALL users to execute ALL commands.","title":"Root User"},{"location":"linux/#creating-groups-and-users","text":"","title":"Creating Groups and Users"},{"location":"linux/#creating-a-group","text":"groupadd groupname Reason: mostly for file sharing. Groupname considerations (to prevent issues): The first character: underscore _ or a lower-case letter a-z. Max 16 (can be up to 32 but may give issues on some distributions) remaining characters can be alphanumeric, the dash - or an underscore_. The last character should not be a hyphen -. example: hannes","title":"Creating a Group"},{"location":"linux/#modifying-a-group","text":"groupmod groupmod -n newname oldname -- change group name groupmod -g newid groupname -- change group id Changing the group name will have no impact on file access because the id is used as the identifier for the group. Changing the GID however will cause all associated files to become 'orphaned'. They will have a reference to a GID and no Group Name. Orphaned files can be found with find / -nogroup","title":"Modifying a group"},{"location":"linux/#deleting-a-group","text":"groupdel groupname This will also cause 'orphaned' files. Only supplemental groups can be deleted.","title":"Deleting a group"},{"location":"linux/#creating-a-user","text":"Command useradd Example useradd -u 1000 -g users -G wheel,research -c 'Jane Doe' jane Can be done manually, but is unsafe due to errors. In some distributions, creating a new users also creates a User Private Group (UPG) with the same name as the user. useradd will use some default settings to create new users. useradd -D view or change these default settings. (/etc/default/useradd file) Example output: GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes GROUP : default primary group (in distributions not using UPG's. Is normally the 'users' group) HOME : home directory INACTIVE : number of days after the password expires. (-1 means disabled) EXPIRE : expiration date SHELL : default shell SKEL : skeleton directory, can be used to populate default files and folder in the home directory of the user. CREATE_MAIL_SPOOL : local file where incoming email is placed. Other default values can be found in /etc/login.defs such as the mail directory, password max days, password min days, password min length, pasword warning age, uid min, uid max, gid min, gid max, umask etc.","title":"Creating a User"},{"location":"linux/#advantages-for-multiple-users","text":"selective access to certain files for certain users sudo command can be configured for selective commands and will log the usage by each user group membership for greater manamgement flexibility","title":"Advantages for multiple users"},{"location":"linux/#changing-a-password","text":"passwd -> for own account passwd username -> for other account","title":"Changing a password"},{"location":"linux/#changing-an-existing-user-account","text":"usermod username usermod -c comment username : comment usermod -d directory username : home directory usermod -f days username : inactive usermod -e expire_date username : expiration date usermod -g groupname username : set primary group usermod -G groupname,groupname username : set supplemental groups usermod -a groupname username : add supplemental groups usermod -l loginname username : set new login name usermod -L username : lock the user account usermod -s shell username : set the login shell usermod -u user_id username : set net userid usermod -U username : unlock the account","title":"Changing an existing user account"},{"location":"linux/#deleting-a-user","text":"userdel username : deletes the user but keeps the user's home directory userdel -r username : deleted user and home directory","title":"Deleting a user"},{"location":"linux/#ownership-and-permissions","text":"Each file has a user owner and a group owner. By default the creator of the file and the primary group of this account are the owners of the file. groups -- shows the groups of which the current user is part of newgrp groupname -- temporarily change the primary group to one of the supplemental groups (to create a file with different group permissions) chgrp groupname filename -- changes group ownership of an existing file. -R option for recursive change of directory stat filename -- shows detailed information about a file (group ownership etc.) chown username filename -- change user ownership chown username:groupname filename -- change user and group ownership chown :groupname filename -- change group ownership","title":"Ownership and permissions"},{"location":"linux/#permissions","text":"Source","title":"Permissions"},{"location":"linux/#viewing-permissions","text":"Permissons are managed by rwx bits. -r read -w write -x execute Read: files can be read and copied. Directories can be listed without details. Write: Files can be written to and saved. Files can be added or remove from a directory. Read is necessary for correct usage. Execute: file can be executed or run. Directory can be changed by cd and directory can be used in path for commands. The permissions of all parent directories must be considered before considering the permissions on a specific file. If a user only has read access to a parent directory of a file, he cannot access the file. The w permission allows a user to delete files from a directory, but only if the user also has x permission on the directory. You can view these permissions by following command: ls -l filename.txt The permissions are managed for three different users/groups: u: User g: Group o: Others (all) Example output: -rw-rw-r-- 1 home home can be interpreted as read/write for home user, read/write for home group, read for all. Permissions are only valid for the contents of the file, not the file itself. Renaming a file for example looks at the permissions for the upper directory, instead of the file itself. Default permissions for files are not executable, but for directories are executable. For directories, this needs to be set to be able to call the ls command on the file.","title":"Viewing permissions"},{"location":"linux/#changing-permissions","text":"","title":"Changing permissions"},{"location":"linux/#chmod","text":"chmod options permissions file name Example symbolic mode: chmod u=rwx,g=rx,o=r myfile Example numeric mode: chmod 754 myfile Numeric mode: 4: \"read\" 2: \"write\" 1: \"execute\" 0: \"no permission.\" So 7 is the combination of permissions 4+2+1 (read, write, and execute), 5 is 4+0+1 (read, no write, and execute), and 4 is 4+0+0 (read, no write, and no execute). Symbolic mode: u: \"user\" g: \"group\" o: \"others\" a: \"all (user, group, other)\" +: add permission -: remove permission =: set exact permission r: read w: write x: execute s: setuid/setgid Example: add read permission for user on file abc.txt chmod u+r abc.txt chmod ug+r,o-w abc.txt -- add read for user owner and group owner, remove write from others","title":"Chmod"},{"location":"linux/#umask","text":"Umask (user file-creation mode mask) is a default set of permissions for a user when he/she created a file or new directory. The umask is defined by three numbers, and is subtracted from the default settings of 777 for directories and 666 for files. A umask of 002 would create directories with permission of 775 and files of 664. A high umask means a safer environment. umask -- shows the umask of the current user, ex. 0002 The first number indicates that the umask is given as an octal number. Permanently changing a user's umask requires modifying the .bashrc file located in that user's home directory.","title":"Umask"},{"location":"linux/#file-type","text":"ls -l /etc/passwd -rw-r--r--. 1 root root 4135 May 27 21:08 /etc/passwd First character points to the file type: * - :regular file * d :directory file * l :link (pointer to other file) * b :block file - relates to block hardware device where data is read in blocks * c :character file - read one byte at a time * p :pipe file * s :socker file - allows two processes o communicate","title":"File type"},{"location":"linux/#special-permissions-links-and-file-locations","text":"","title":"Special permissions, Links and File locations"},{"location":"linux/#setuid-permission-s-or-4","text":"Setuid allows a binary file to be run as the owner of the file instead of the user who executes the file. Some system utilities use this setting to allow a user to run a file with root priviledges. chmod u+s filename -- adds the setuid permission chmod 4*** filename -- adds the setuid numerically (replace * by the numeric permissions the file already has)","title":"SETUID permission (s or 4)"},{"location":"linux/#setgid-permission","text":"Similar as the setuid permission, but for groups. Behavior depends on if the permission is set to a file or a directory. setgid on a file will allow a user to run an executable binary as temp member of the group. Example: /usr/bin/wall as executable with permissions: -rwxr-sr-x. 1 root tty 10996 Jul 19 2011 /usr/bin/wall The s permission on the group allows the user to execute this executable as member of the tty group and thus also access all necessery files to execute this command. setgid on a directory causes files that are created in the directory to be owner by the group that owns the directory. Normally they are owned by the primary group of the user. The newly created directories will also have the same setgid permission. This is necessary for team work if users with different primary groups need to create new files and directories in a shared directory.","title":"SETGID permission"},{"location":"linux/#sticky-bit-permission-t-or-1","text":"Prevents other users from deleting your files in a shared directory. Only the owner will be able to delete the file. Symbolic: chmod o+t directoryname Numeric: chmod 1775 filename/directoryname (add 1 before the existing permission) lowercase t - both sticky bit and execution are set for 'others' uppercate T - only sticky bit is set for 'others'","title":"Sticky Bit permission (t or 1)"},{"location":"linux/#links","text":"Hard to access files (with long url's, deeply buried in directories) can be 'copied' to a link file. This file acts as a shorcut to the file. ls -li filename - shows number of existing links for a file ln existingfilename newfilename - creates a hard link ln -s existingfilename newfilename - creates a soft link Hard link will create file which points to same inode. Soft link will point to filename. Deleting 'hard link' file will only work if all files which point to the same inode are deleted. -> safer Soft link works with directories. Soft link can work between different file systems (partitions). Difference between hard and soft links: Underneath the file system files are represented by inodes. A file in the file system is basically a link to an inode. A hard link then just creates another file with a link to the same underlying inode. When you delete a file it removes one link to the underlying inode. The inode is only deleted (or deletable/over-writable) when all links to the inode have been deleted. A symbolic link is a link to another name in the file system. Once a hard link has been made the link is to the inode. deleting renaming or moving the original file will not affect the hard link as it links to the underlying inode. Any changes to the data on the inode is reflected in all files that refer to that inode. Note: Hard links are only valid within the same File System. Symbolic links can span file systems as they are simply the name of another file.","title":"Links"},{"location":"mkdocs/","text":"MkDocs Generates static sites based on markdown files. https://www.mkdocs.org mkdocs serve - Start the live-reloading docs server. Launch in directory of .yml file. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploys the pages to Github Project Pages Markdown sheat-sheet: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#lines Live Page","title":"MkDocs"},{"location":"mkdocs/#mkdocs","text":"Generates static sites based on markdown files. https://www.mkdocs.org mkdocs serve - Start the live-reloading docs server. Launch in directory of .yml file. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploys the pages to Github Project Pages Markdown sheat-sheet: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#lines Live Page","title":"MkDocs"},{"location":"netwerkbeheer/","text":"Netwerkbeheer Nieuwe VM aanmaken Copy template to new map Rename template and map Add new VM - choose template when asked DHCP Dynamic Host Configuration Protocol. Zorgt voor de verdeling van IP-adressen . Elke DHCP-server heeft een pool van vrije IP-adressen om uit te delen, en verdeelt deze met een bepaalde lease-time, waarna deze opnieuw worden vrijgegeven. Voordelen dynamische ip-adressen: Geen ip-configuratie nodig voor elk toestel, kan centraal beheerd worden. Effici\u00ebnter omgaan met IP-adressen. Beperkt hierdoor kosten. Proces om IP-adres te verkrijgen loopt via DORA Discover : client verstuurt broadcast naar alle computers binnen Ethernet-segment. Offer : server biedt vrij IP-adres aan. Request : client doet werkelijke aanvraag voor dit IP-adres (bij \u00e9\u00e9n DHCP-server, andere DHCP-servers zien via broadcast dat hun aanbod niet nodig is) Acknowledge : server bevestigt de aanvraag Create new DHCP Server DNS Domain Name System/Server zorgt voor vertaling tussen IP-adres en naam . De DNS-Server houdt een tabel bij met Resource Records: A : Address records - hostname to ip-address AAAA : IPv6 Address records - hostname to IPv6-address CNAME : Alias - hostname to other hostname (redirects) MX : Mail Exchange record - specifiec mail server Proces DNS request surf naar www.google.com -> zoekt ip adres Twee lokale opzoekingen, daarna neemt uw dns server het over. (lokaal is dat dns server van bv. telenet) check etc/hosts daarna dns cache daarna vraagt hij aan dns server in domein. Indien DNS server in domein adres niet heeft root DNS Server .com google.com drive.google.com Active Directory Algemeen Kan niet zonder DNS! Voor AD moest je op elke PC een gebruiker aanmaken om te kunnen authenticeren. Authenticatie gaat via Kerberos op de Domain Controller (waar AD staat). Kerberos stuurt tickets voor toegang -> zie youtube video. Kleine bedrijven: kan via Azure AD, voor grote bedrijven niet voldoende (geen LDAP ondersteuning bvb.). Kan onder meerdere domeinen (subdomeinen dan eigenlijk). Maar geeft problemen voor authenticatie in cloud, dus meer en meer \u00e9\u00e9n groot domein. Functie Domein Controller(s): Host databank van AD Repliceren elkaar (niet zoals bij DNS waarbij \u00e9\u00e9n hoofd DNS (primary zone) en een backup DNS (secondary zone) Installatie Indien DNS en AD DC zelfde server zijn: * Wijzig naam server en herstart * Add role * Kies DNS erbij indien nodig * Zet IP-address en wijs DNS-server naar eigen IP-address! Group Policies Settings voor meerdere gebruikers of computers. Twee verschillende zaken: Policies: afschermen van zaken Preferences: settings die de gebruiker kan het aanpassen Client driven: de client vraagt aan de server welke policies hij/zij moet volgen Inheritence : overerving van policies (kan afgezet worden). Wordt aangegeven door uitroepteken. Enforce : wordt steeds toegepast (ook zonder inheritence). Wordt aangegeven door slotje. Steeds groeperen per functie. Bvb. alle rdp policies groeperen. Scheiding van gebruiker/groepen. Domain Controller configureren geef ip adres: 10.10 installeer rollen maak domeincontroller: cursusdom.be 5 rollen: (FSMO) (kunnen bij \u00e9\u00e9n domeincontroller) RID Master: verdeeld groepen van security identifiers aan de andere domein controllers: zijn deze op moet je opnieuw bij de RID master nieuwe keys ophalen. PDC emulator: distributie van verandering naar andere domein controllers Schema Master: houdt bij hoe u database er uit ziet. Domein Naming Master: Infrastructure Master: PC toevoegen aan domain !Moet altijd vanuit Client! To join a computer to a domain On the Start screen, typeControl Panel, and then press ENTER. Navigate to System and Security, and then click System. Under Computer name, domain, and workgroup settings, click Change settings. On the Computer Name tab, click Change. Under Member of, click Domain, type the name of the domain that this computer will join, and then click OK. Click OK, and then restart the computer. DONT RENAME AND ADD TO DOMAIN AT THE SAME TIME Via powershell: Eerst moet je DNS Server configureren, anders geraak je niet aan het domein netsh interface ipv4 add dnsserver \"Ethernet\" address=192.168.x.x index=1 Met volgende command kan je de lokale pc toevoegen aan het domein. Hierbij moet je je authenticeren met een account van het domein. netdom join %THE_COMPUTER_NAME% /domain:OPSCODEDEMO.COM /userd:Administrator /passwordd:xxx Mogelijke problemen: IP instellingen: fixed IP DNS Server Gateway instellingen Gekoppeld aan virtuele switch? Firewall? Pingen indien geen reply en bovenstaande is ok -> Firewall staat op Check of DNS werkt. (ping domein ipv ip-adres of NSLookup) AGDLP Account Global Group Domain Local Group Permission Account Global Group Domain Local Group Permission (use NTFS) Jos.Smet GG_HR DL_Facturen_R(read) centered DL_Facturen_W(write) Facturen (share map) are neat DL_Facturen_FC(fullctr) Bij folder sharen: Share altijd everyone full control en dan per gebruiker/group met NTFS Inheritance altijd uit Op examen niet rechtstreeks, maar altijd met groep structuur werken: OU: cursusdom OU: groepen/users/computers/servers groepen: Global Groups GG_HR, etc. Voordeel van computer toe te voegen op domein: iedere gebruiker binnen domein kan aanmelden via die pc pc kan beheerd worden vanaf netwerk Domain Administrator Difference between local admin and domain admin Before a Domain Controller is promoted to that role, it is a simple workgroup (standalone) server and has a local Administrator account and a local Administrators group. When you create a domain, those accounts don't go away; they're incorporated into the domain as the domain Administrator account and the domain builtin\\Administrators group. The builtin\\Administrators group has Administrative access to the Domain Controllers, but is not automatically granted administrative access to all computers within the domain, whereas Domain Admins are. WIN10 Client Computername: CL10 Username: Local_Admin Password: R1234-56 Fileserver Kan rechtstreeks (via GUI) via system - Kan eenvoudiger via de Domein Controller Server Manager All Servers Server toevoegen via zoekfunctie Functies en rollen installeren. start powershell Install-WindowsFeature \u2013Name FS-Resource-Manager Folder C:DATA sharen Beter via Server manager: fileserver - shares - tasks - new New-SMBShare \u2013Name \"Shared\" \u2013Path \"C:\\Shared\" \u2013ContinuouslyAvailable \u2013FullAccess domain\\admingroup ` Test via nieuwe netwerklocatie aan te maken: \\ServerCore\\Data Kan ook gewoon via \\ServerCore -> laat alle shared mappen zien Powershell Syntax $PSItem or $_ Gets the item in a foreach or after the pipeline. $ (dollar underscore) 'THIS' token. Typically refers to the item inside a foreach loop. Task: Print all items in a collection. Solution. ... | foreach { Write-Host $ } get-process | where-object {$_.cpu -gt 1 -and $_.company -ne 'Microsoft Corporation'} | ft name, cpu, company Comparators -eq Equal -ne Not equal -ge Greater than or equal -gt Greater than -lt Less than -le Less than or equal -like Wildcard comparison -notlike Wildcard comparison -match Regular expression comparison -notmatch Regular expression comparison -replace Replace operator -contains Containment operator -notcontains Containment operator -in Like \u2013contains, but with the operands reversed.(PowerShell 3.0) -notin Like \u2013notcontains, but with the operands reversed.(PowerShell 3.0) get-childitem | c:\\windows | where-object LastAccessTime -gt (Get-Date).AddMonths(-1) Useful commands Get-process get-process --- toont alle processen get-process -name notepad --- toont running notepad processen get-process -name n* --- toont alle processen starting with N Stop-process stop-process -name notepad --- stop all notepad processen stop-process -name notepad -whatif --- check which processes would have been stopped Get-Date Shows current date. (Get-Date).DayOfWeek --- dag van de week (Get-Date).AddYears/AddDays/ --- add amount of time to dat Get-Date -Day 1 -Month 7 -Year (Get-Date).Year --- toont op welke dag 1 juli van het huidig jaar valt Oefing Core Veel instellingen kunnen ook via sconfig naam wijzigen get current computername: hostname wmic computersystem where caption='xp-pc' rename windows7-pc ip aanpassen get name of interface: netsh interface show interface change ip adress: netsh interface ip set address \"Ethernet\" static 10.0.0.100 255.255.0.0 10.0.0.1 1 show change: netsh interface show interface windows update uitzetten BETER via powershell: stop-service wuauserv set-service wuauserv -StartupType Disabled firewall uitzetten netsh advfirewall set allprofiles state off","title":"Netwerkbeheer"},{"location":"netwerkbeheer/#netwerkbeheer","text":"","title":"Netwerkbeheer"},{"location":"netwerkbeheer/#nieuwe-vm-aanmaken","text":"Copy template to new map Rename template and map Add new VM - choose template when asked","title":"Nieuwe VM aanmaken"},{"location":"netwerkbeheer/#dhcp","text":"Dynamic Host Configuration Protocol. Zorgt voor de verdeling van IP-adressen . Elke DHCP-server heeft een pool van vrije IP-adressen om uit te delen, en verdeelt deze met een bepaalde lease-time, waarna deze opnieuw worden vrijgegeven. Voordelen dynamische ip-adressen: Geen ip-configuratie nodig voor elk toestel, kan centraal beheerd worden. Effici\u00ebnter omgaan met IP-adressen. Beperkt hierdoor kosten. Proces om IP-adres te verkrijgen loopt via DORA Discover : client verstuurt broadcast naar alle computers binnen Ethernet-segment. Offer : server biedt vrij IP-adres aan. Request : client doet werkelijke aanvraag voor dit IP-adres (bij \u00e9\u00e9n DHCP-server, andere DHCP-servers zien via broadcast dat hun aanbod niet nodig is) Acknowledge : server bevestigt de aanvraag","title":"DHCP"},{"location":"netwerkbeheer/#create-new-dhcp-server","text":"","title":"Create new DHCP Server"},{"location":"netwerkbeheer/#dns","text":"Domain Name System/Server zorgt voor vertaling tussen IP-adres en naam . De DNS-Server houdt een tabel bij met Resource Records: A : Address records - hostname to ip-address AAAA : IPv6 Address records - hostname to IPv6-address CNAME : Alias - hostname to other hostname (redirects) MX : Mail Exchange record - specifiec mail server","title":"DNS"},{"location":"netwerkbeheer/#proces-dns-request","text":"surf naar www.google.com -> zoekt ip adres Twee lokale opzoekingen, daarna neemt uw dns server het over. (lokaal is dat dns server van bv. telenet) check etc/hosts daarna dns cache daarna vraagt hij aan dns server in domein. Indien DNS server in domein adres niet heeft root DNS Server .com google.com drive.google.com","title":"Proces DNS request"},{"location":"netwerkbeheer/#active-directory","text":"","title":"Active Directory"},{"location":"netwerkbeheer/#algemeen","text":"Kan niet zonder DNS! Voor AD moest je op elke PC een gebruiker aanmaken om te kunnen authenticeren. Authenticatie gaat via Kerberos op de Domain Controller (waar AD staat). Kerberos stuurt tickets voor toegang -> zie youtube video. Kleine bedrijven: kan via Azure AD, voor grote bedrijven niet voldoende (geen LDAP ondersteuning bvb.). Kan onder meerdere domeinen (subdomeinen dan eigenlijk). Maar geeft problemen voor authenticatie in cloud, dus meer en meer \u00e9\u00e9n groot domein. Functie Domein Controller(s): Host databank van AD Repliceren elkaar (niet zoals bij DNS waarbij \u00e9\u00e9n hoofd DNS (primary zone) en een backup DNS (secondary zone)","title":"Algemeen"},{"location":"netwerkbeheer/#installatie","text":"Indien DNS en AD DC zelfde server zijn: * Wijzig naam server en herstart * Add role * Kies DNS erbij indien nodig * Zet IP-address en wijs DNS-server naar eigen IP-address!","title":"Installatie"},{"location":"netwerkbeheer/#group-policies","text":"Settings voor meerdere gebruikers of computers. Twee verschillende zaken: Policies: afschermen van zaken Preferences: settings die de gebruiker kan het aanpassen Client driven: de client vraagt aan de server welke policies hij/zij moet volgen Inheritence : overerving van policies (kan afgezet worden). Wordt aangegeven door uitroepteken. Enforce : wordt steeds toegepast (ook zonder inheritence). Wordt aangegeven door slotje. Steeds groeperen per functie. Bvb. alle rdp policies groeperen. Scheiding van gebruiker/groepen.","title":"Group Policies"},{"location":"netwerkbeheer/#domain-controller-configureren","text":"geef ip adres: 10.10 installeer rollen maak domeincontroller: cursusdom.be 5 rollen: (FSMO) (kunnen bij \u00e9\u00e9n domeincontroller) RID Master: verdeeld groepen van security identifiers aan de andere domein controllers: zijn deze op moet je opnieuw bij de RID master nieuwe keys ophalen. PDC emulator: distributie van verandering naar andere domein controllers Schema Master: houdt bij hoe u database er uit ziet. Domein Naming Master: Infrastructure Master:","title":"Domain Controller configureren"},{"location":"netwerkbeheer/#pc-toevoegen-aan-domain","text":"!Moet altijd vanuit Client! To join a computer to a domain On the Start screen, typeControl Panel, and then press ENTER. Navigate to System and Security, and then click System. Under Computer name, domain, and workgroup settings, click Change settings. On the Computer Name tab, click Change. Under Member of, click Domain, type the name of the domain that this computer will join, and then click OK. Click OK, and then restart the computer. DONT RENAME AND ADD TO DOMAIN AT THE SAME TIME Via powershell: Eerst moet je DNS Server configureren, anders geraak je niet aan het domein netsh interface ipv4 add dnsserver \"Ethernet\" address=192.168.x.x index=1 Met volgende command kan je de lokale pc toevoegen aan het domein. Hierbij moet je je authenticeren met een account van het domein. netdom join %THE_COMPUTER_NAME% /domain:OPSCODEDEMO.COM /userd:Administrator /passwordd:xxx","title":"PC toevoegen aan domain"},{"location":"netwerkbeheer/#mogelijke-problemen","text":"IP instellingen: fixed IP DNS Server Gateway instellingen Gekoppeld aan virtuele switch? Firewall? Pingen indien geen reply en bovenstaande is ok -> Firewall staat op Check of DNS werkt. (ping domein ipv ip-adres of NSLookup)","title":"Mogelijke problemen:"},{"location":"netwerkbeheer/#agdlp","text":"Account Global Group Domain Local Group Permission Account Global Group Domain Local Group Permission (use NTFS) Jos.Smet GG_HR DL_Facturen_R(read) centered DL_Facturen_W(write) Facturen (share map) are neat DL_Facturen_FC(fullctr) Bij folder sharen: Share altijd everyone full control en dan per gebruiker/group met NTFS Inheritance altijd uit Op examen niet rechtstreeks, maar altijd met groep structuur werken: OU: cursusdom OU: groepen/users/computers/servers groepen: Global Groups GG_HR, etc. Voordeel van computer toe te voegen op domein: iedere gebruiker binnen domein kan aanmelden via die pc pc kan beheerd worden vanaf netwerk","title":"AGDLP"},{"location":"netwerkbeheer/#domain-administrator","text":"","title":"Domain Administrator"},{"location":"netwerkbeheer/#difference-between-local-admin-and-domain-admin","text":"Before a Domain Controller is promoted to that role, it is a simple workgroup (standalone) server and has a local Administrator account and a local Administrators group. When you create a domain, those accounts don't go away; they're incorporated into the domain as the domain Administrator account and the domain builtin\\Administrators group. The builtin\\Administrators group has Administrative access to the Domain Controllers, but is not automatically granted administrative access to all computers within the domain, whereas Domain Admins are.","title":"Difference between local admin and domain admin"},{"location":"netwerkbeheer/#win10-client","text":"Computername: CL10 Username: Local_Admin Password: R1234-56","title":"WIN10 Client"},{"location":"netwerkbeheer/#fileserver","text":"Kan rechtstreeks (via GUI) via system - Kan eenvoudiger via de Domein Controller Server Manager All Servers Server toevoegen via zoekfunctie Functies en rollen installeren. start powershell Install-WindowsFeature \u2013Name FS-Resource-Manager Folder C:DATA sharen Beter via Server manager: fileserver - shares - tasks - new New-SMBShare \u2013Name \"Shared\" \u2013Path \"C:\\Shared\" \u2013ContinuouslyAvailable \u2013FullAccess domain\\admingroup ` Test via nieuwe netwerklocatie aan te maken: \\ServerCore\\Data Kan ook gewoon via \\ServerCore -> laat alle shared mappen zien","title":"Fileserver"},{"location":"netwerkbeheer/#powershell","text":"","title":"Powershell"},{"location":"netwerkbeheer/#syntax","text":"","title":"Syntax"},{"location":"netwerkbeheer/#psitem-or-_","text":"Gets the item in a foreach or after the pipeline. $ (dollar underscore) 'THIS' token. Typically refers to the item inside a foreach loop. Task: Print all items in a collection. Solution. ... | foreach { Write-Host $ } get-process | where-object {$_.cpu -gt 1 -and $_.company -ne 'Microsoft Corporation'} | ft name, cpu, company","title":"$PSItem or $_"},{"location":"netwerkbeheer/#comparators","text":"-eq Equal -ne Not equal -ge Greater than or equal -gt Greater than -lt Less than -le Less than or equal -like Wildcard comparison -notlike Wildcard comparison -match Regular expression comparison -notmatch Regular expression comparison -replace Replace operator -contains Containment operator -notcontains Containment operator -in Like \u2013contains, but with the operands reversed.(PowerShell 3.0) -notin Like \u2013notcontains, but with the operands reversed.(PowerShell 3.0) get-childitem | c:\\windows | where-object LastAccessTime -gt (Get-Date).AddMonths(-1)","title":"Comparators"},{"location":"netwerkbeheer/#useful-commands","text":"","title":"Useful commands"},{"location":"netwerkbeheer/#get-process","text":"get-process --- toont alle processen get-process -name notepad --- toont running notepad processen get-process -name n* --- toont alle processen starting with N","title":"Get-process"},{"location":"netwerkbeheer/#stop-process","text":"stop-process -name notepad --- stop all notepad processen stop-process -name notepad -whatif --- check which processes would have been stopped","title":"Stop-process"},{"location":"netwerkbeheer/#get-date","text":"Shows current date. (Get-Date).DayOfWeek --- dag van de week (Get-Date).AddYears/AddDays/ --- add amount of time to dat Get-Date -Day 1 -Month 7 -Year (Get-Date).Year --- toont op welke dag 1 juli van het huidig jaar valt Oefing Core Veel instellingen kunnen ook via sconfig naam wijzigen get current computername: hostname wmic computersystem where caption='xp-pc' rename windows7-pc ip aanpassen get name of interface: netsh interface show interface change ip adress: netsh interface ip set address \"Ethernet\" static 10.0.0.100 255.255.0.0 10.0.0.1 1 show change: netsh interface show interface windows update uitzetten BETER via powershell: stop-service wuauserv set-service wuauserv -StartupType Disabled firewall uitzetten netsh advfirewall set allprofiles state off","title":"Get-Date"},{"location":"rabbitmq/","text":"RabbitMQ Overview To prevent too many connections from opening, only one connection is opened, and channels are logically divided to run on that one connection. - one physical connection - multiple 'logical' connections -> channels Exchanges Direct -> routing key to a single queue Fanout -> dispatch to all queues bound to this exchange without routing key Topic -> routing key with regex like routing Header Queues Persistant -> data is kept on restart (stored phyically) Transient -> data is lost on restart Message TTL: can be declared by developers on publish, or inherited by queue. Default is no end time to live. Consumer Acknowledge How to make sure message is succesfully produced or consumed. Producer Transaction: callback after commit sending multiple messages Notification: (on single message) Consumer Manual acknowledge: in code explicit (automatically in MassTransit,, together with retry) Automatic acknowledge: Virtual Host Name - to differentiate between different environments, each with its own credentials. Virtual hosts for different clients - to seperate different client needs. Each has its own user, exchanges and queues. Scaling Clustering Node - installation of rabbitMq. Cluster contains multiple nodes (or installations). To avoid single point of failure. Can be automatically or manually build. (join cluster wit cmd) -> queues (definition) is replicated, data is not automatically replicated. -> apply a policy to enforce data replication - mirrored queue -> data is consumed from one master node -> load balancer needed as intermediate layer, to shield the producer from referencing mutliple nodes (with ip and port etc). Good from raising Availability, but lowers Performance. RabbitMQ Federation/Shovel Federation: possibility to produce to one cluster, but consume from another cluster. Source and destination cluster. Federation link between the clusters (queues or exchanges). Also usefull for linking data between two physical location (for complete disaster failure of one cluster). Read without remove Negative acknowledge is possible - reads but not removes from queue. HttpPort: 15672 voor GUI AMQP-port: 5672 MassTransit: Command vs Event Command with one consumer Event with multiple consumers Sending or Publishing will generate an Exchange Consuming will generate an exchange that is coupled to the publisher exchange together with a queue. MassTransit consumer - multiple consumers possible without destroying message on ackn. Masstransit will create a general consumer in the background to publish messages to multiple consumers. (Adds a consumer layer.) Questions: - large or small messages","title":"RabbitMQ"},{"location":"rabbitmq/#rabbitmq","text":"","title":"RabbitMQ"},{"location":"rabbitmq/#overview","text":"To prevent too many connections from opening, only one connection is opened, and channels are logically divided to run on that one connection. - one physical connection - multiple 'logical' connections -> channels","title":"Overview"},{"location":"rabbitmq/#exchanges","text":"Direct -> routing key to a single queue Fanout -> dispatch to all queues bound to this exchange without routing key Topic -> routing key with regex like routing Header","title":"Exchanges"},{"location":"rabbitmq/#queues","text":"Persistant -> data is kept on restart (stored phyically) Transient -> data is lost on restart Message TTL: can be declared by developers on publish, or inherited by queue. Default is no end time to live.","title":"Queues"},{"location":"rabbitmq/#consumer","text":"","title":"Consumer"},{"location":"rabbitmq/#acknowledge","text":"How to make sure message is succesfully produced or consumed. Producer Transaction: callback after commit sending multiple messages Notification: (on single message) Consumer Manual acknowledge: in code explicit (automatically in MassTransit,, together with retry) Automatic acknowledge:","title":"Acknowledge"},{"location":"rabbitmq/#virtual-host","text":"Name - to differentiate between different environments, each with its own credentials. Virtual hosts for different clients - to seperate different client needs. Each has its own user, exchanges and queues.","title":"Virtual Host"},{"location":"rabbitmq/#scaling","text":"","title":"Scaling"},{"location":"rabbitmq/#clustering","text":"Node - installation of rabbitMq. Cluster contains multiple nodes (or installations). To avoid single point of failure. Can be automatically or manually build. (join cluster wit cmd) -> queues (definition) is replicated, data is not automatically replicated. -> apply a policy to enforce data replication - mirrored queue -> data is consumed from one master node -> load balancer needed as intermediate layer, to shield the producer from referencing mutliple nodes (with ip and port etc). Good from raising Availability, but lowers Performance.","title":"Clustering"},{"location":"rabbitmq/#rabbitmq-federationshovel","text":"Federation: possibility to produce to one cluster, but consume from another cluster. Source and destination cluster. Federation link between the clusters (queues or exchanges). Also usefull for linking data between two physical location (for complete disaster failure of one cluster).","title":"RabbitMQ Federation/Shovel"},{"location":"rabbitmq/#read-without-remove","text":"Negative acknowledge is possible - reads but not removes from queue. HttpPort: 15672 voor GUI AMQP-port: 5672","title":"Read without remove"},{"location":"rabbitmq/#masstransit-command-vs-event","text":"Command with one consumer Event with multiple consumers Sending or Publishing will generate an Exchange Consuming will generate an exchange that is coupled to the publisher exchange together with a queue. MassTransit consumer - multiple consumers possible without destroying message on ackn. Masstransit will create a general consumer in the background to publish messages to multiple consumers. (Adds a consumer layer.) Questions: - large or small messages","title":"MassTransit: Command vs Event"},{"location":"soap/","text":"SOAP Simple Object Access Protocol Protocol Communicatie tussen verschillende componenten XML Http, of SMTP, FTP, ...","title":"SOAP"},{"location":"soap/#soap","text":"Simple Object Access Protocol Protocol Communicatie tussen verschillende componenten XML Http, of SMTP, FTP, ...","title":"SOAP"},{"location":"sql/","text":"SQL Overview Source Indexing Indexing is basically sorting your data on a specified column to improve searching by that column. Advantages: Improves performance on reading Disadvantages: Decrease performance when writing data because of page splits Non-clustered indexes need extra physical memory space Clustered index Clustered index is a physical sorting of the data by the chosen column. Therefore you can only have one clustered index per table. A primary key or unique contraint will by default act as a clustered index if you do not specify a clusterd index. Non-clustered index Non-clustered index is an index that is sorted by the chosen column, and keeps a pointer to the row instead of the actual data. You can define multiple non-clustered indices. Pages Microsoft Docs Source When a query is issued against an indexed column, the query engine starts at the root node and navigates down through the intermediate nodes, with each layer of the intermediate level more granular than the one above. The query engine continues down through the index nodes until it reaches the leaf node. For example, if you\u2019re searching for the value 123 in an indexed column, the query engine would first look in the root level to determine which page to reference in the top intermediate level. In this example, the first page points the values 1-100, and the second page, the values 101-200, so the query engine would go to the second page on that level. The query engine would then determine that it must go to the third page at the next intermediate level. From there, the query engine would navigate to the leaf node for value 123. The leaf node will contain either the entire row of data or a pointer to that row, depending on whether the index is clustered or nonclustered. Page Splits and Fill factor Microsoft Docs Source Table and index data is stored in 'pages' of 8kb. Fill-factor will define how much extra space you leave open for future expansion of the index. If the pages are filled by 100 (fillfactor 0 or 100), the creation of an extra row, or the update of an existing index with extra data, will (or might) cause the page to split. This will move 50% of the page to a new page to make room for the new data, and will cause a performance hit. Fill-factor is defined between 1-100, being the percentage to fill the page with. The remainder will be left open between the rows. Considerations: Do not use a fillfactor of less then 100 on a identity column index. New data will always be generated at the end of the last page, so leaving extra space will only take more memory without preventing page-splits. This can only be usefull when you can update the existing identity column with extra data. A smaller fillfactor will reduce requirements for page splits, but will also decrease read performance and increase memory usage. Query optimization Execution plan Order of execution Difference between logical order and the physical execution order. The logical order is defined by your query, the Query Optimizer will use these logical requirements and try to create the most optimal physical execution order. Nested loops","title":"SQL"},{"location":"sql/#sql","text":"Overview Source","title":"SQL"},{"location":"sql/#indexing","text":"Indexing is basically sorting your data on a specified column to improve searching by that column. Advantages: Improves performance on reading Disadvantages: Decrease performance when writing data because of page splits Non-clustered indexes need extra physical memory space","title":"Indexing"},{"location":"sql/#clustered-index","text":"Clustered index is a physical sorting of the data by the chosen column. Therefore you can only have one clustered index per table. A primary key or unique contraint will by default act as a clustered index if you do not specify a clusterd index.","title":"Clustered index"},{"location":"sql/#non-clustered-index","text":"Non-clustered index is an index that is sorted by the chosen column, and keeps a pointer to the row instead of the actual data. You can define multiple non-clustered indices.","title":"Non-clustered index"},{"location":"sql/#pages","text":"Microsoft Docs Source When a query is issued against an indexed column, the query engine starts at the root node and navigates down through the intermediate nodes, with each layer of the intermediate level more granular than the one above. The query engine continues down through the index nodes until it reaches the leaf node. For example, if you\u2019re searching for the value 123 in an indexed column, the query engine would first look in the root level to determine which page to reference in the top intermediate level. In this example, the first page points the values 1-100, and the second page, the values 101-200, so the query engine would go to the second page on that level. The query engine would then determine that it must go to the third page at the next intermediate level. From there, the query engine would navigate to the leaf node for value 123. The leaf node will contain either the entire row of data or a pointer to that row, depending on whether the index is clustered or nonclustered.","title":"Pages"},{"location":"sql/#page-splits-and-fill-factor","text":"Microsoft Docs Source Table and index data is stored in 'pages' of 8kb. Fill-factor will define how much extra space you leave open for future expansion of the index. If the pages are filled by 100 (fillfactor 0 or 100), the creation of an extra row, or the update of an existing index with extra data, will (or might) cause the page to split. This will move 50% of the page to a new page to make room for the new data, and will cause a performance hit. Fill-factor is defined between 1-100, being the percentage to fill the page with. The remainder will be left open between the rows. Considerations: Do not use a fillfactor of less then 100 on a identity column index. New data will always be generated at the end of the last page, so leaving extra space will only take more memory without preventing page-splits. This can only be usefull when you can update the existing identity column with extra data. A smaller fillfactor will reduce requirements for page splits, but will also decrease read performance and increase memory usage.","title":"Page Splits and Fill factor"},{"location":"sql/#query-optimization","text":"","title":"Query optimization"},{"location":"sql/#execution-plan","text":"","title":"Execution plan"},{"location":"sql/#order-of-execution","text":"Difference between logical order and the physical execution order. The logical order is defined by your query, the Query Optimizer will use these logical requirements and try to create the most optimal physical execution order.","title":"Order of execution"},{"location":"sql/#nested-loops","text":"","title":"Nested loops"},{"location":"web_api/","text":"REST API Return types OVERZICHT Link to Microsoft Documentations POST 201 Created 400 Bad Request Return the path to get the created item. You can also include the created item in the body. return CreatedAtAction(nameof(GetById), new { id = product.Id }, product); PUT/PATCH 204 NoContent 400 Bad Request 404 Not Found GET BY ID 200 Ok 404 Not Found GET ALL 200 Ok (returns empty list if no objects are found)","title":"REST API"},{"location":"web_api/#rest-api","text":"","title":"REST API"},{"location":"web_api/#return-types","text":"OVERZICHT Link to Microsoft Documentations","title":"Return types"},{"location":"web_api/#post","text":"201 Created 400 Bad Request Return the path to get the created item. You can also include the created item in the body. return CreatedAtAction(nameof(GetById), new { id = product.Id }, product);","title":"POST"},{"location":"web_api/#putpatch","text":"204 NoContent 400 Bad Request 404 Not Found","title":"PUT/PATCH"},{"location":"web_api/#get-by-id","text":"200 Ok 404 Not Found","title":"GET BY ID"},{"location":"web_api/#get-all","text":"200 Ok (returns empty list if no objects are found)","title":"GET ALL"},{"location":"webhosting/","text":"There are large differences between Web Server Software and the way they handle HttpRequests. This article covers Internet Information Services. Hosting ASP.NET Core in IIS https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/iis/?view=aspnetcore-2.2 IIS and Application Pools Explanation https://stackify.com/iis-web-server/ IIS Web servers handle requests in different ways. Two main ways are to: handle all requests on a single thread (with worker threads, e.g. Node.js) spawn a new thread for each request (IIS) Application Pools A single application pool creates at least one worker process (w3wp.exe). This process runs an instance of your application and handles the request send to your web server. (These application pools are visible in IIS Manager). This process runs your application code or returnes a static page. IIS creates a virtual user for each app pool. These users can also be viewed in your file explorer, as they each have their own folders (music etc.). App pools recycle by default every 29 hours or when a config file changes. This recycling frees up memory used by runaway processes. Advantage of using application pools An Application Pool can contain one or more applications and allows us to configure a level of isolation between different Web applications. For example, if you want to isolate all the Web applications running in the same computer, you can do this by creating a separate application pool for every Web application and placing them in their corresponding application pool. Because each application pool runs in its own worker process, errors in one application pool will not affect the applications running in other application pools . HttpRequest Lifecycle Web Applications require a lot of technologies to build our code .When the user interacts with the app causes several HTTP request to run parallel. The http request life cycle is described as below:- Web Browser submits the request to the server after creating a socket in local progress. The http server waits for the request to come on socket i.e. port-80. The web browser translates the yahoo.com to an IP address if it does not knows it. If the web browser does not recognize the address it contacts a DNS Server to resolve the name. The browser will open a TCP connection to the IP address of yahoo.com and send a HTTP GET request over. The request is then submitted to DNS server where IP address is fetched based on domain name. Now the request is submitted to Http Server as per HTTP protocol. Http server receives the request and shifts the client to another socket. So the socket on port-80 is released to receive request from other clients. Web Browser & server both are connected to each other. The server will process the request, renders the response & breaks the connection. More information on https://docs.microsoft.com/en-us/iis/get-started/introduction-to-iis/introduction-to-iis-architecture#http-request-processing-in-iis When a client browser initiates an HTTP request for a resource on the Web server, HTTP.sys intercepts the request. HTTP.sys contacts WAS to obtain information from the configuration store. WAS requests configuration information from the configuration store, applicationHost.config. The WWW Service receives configuration information, such as application pool and site configuration. The WWW Service uses the configuration information to configure HTTP.sys. WAS starts a worker process for the application pool to which the request was made. The worker process processes the request and returns a response to HTTP.sys. The client receives a response.","title":"WebHosting"},{"location":"webhosting/#iis","text":"Web servers handle requests in different ways. Two main ways are to: handle all requests on a single thread (with worker threads, e.g. Node.js) spawn a new thread for each request (IIS)","title":"IIS"},{"location":"webhosting/#application-pools","text":"A single application pool creates at least one worker process (w3wp.exe). This process runs an instance of your application and handles the request send to your web server. (These application pools are visible in IIS Manager). This process runs your application code or returnes a static page. IIS creates a virtual user for each app pool. These users can also be viewed in your file explorer, as they each have their own folders (music etc.). App pools recycle by default every 29 hours or when a config file changes. This recycling frees up memory used by runaway processes. Advantage of using application pools An Application Pool can contain one or more applications and allows us to configure a level of isolation between different Web applications. For example, if you want to isolate all the Web applications running in the same computer, you can do this by creating a separate application pool for every Web application and placing them in their corresponding application pool. Because each application pool runs in its own worker process, errors in one application pool will not affect the applications running in other application pools .","title":"Application Pools"},{"location":"webhosting/#httprequest-lifecycle","text":"Web Applications require a lot of technologies to build our code .When the user interacts with the app causes several HTTP request to run parallel. The http request life cycle is described as below:- Web Browser submits the request to the server after creating a socket in local progress. The http server waits for the request to come on socket i.e. port-80. The web browser translates the yahoo.com to an IP address if it does not knows it. If the web browser does not recognize the address it contacts a DNS Server to resolve the name. The browser will open a TCP connection to the IP address of yahoo.com and send a HTTP GET request over. The request is then submitted to DNS server where IP address is fetched based on domain name. Now the request is submitted to Http Server as per HTTP protocol. Http server receives the request and shifts the client to another socket. So the socket on port-80 is released to receive request from other clients. Web Browser & server both are connected to each other. The server will process the request, renders the response & breaks the connection. More information on https://docs.microsoft.com/en-us/iis/get-started/introduction-to-iis/introduction-to-iis-architecture#http-request-processing-in-iis When a client browser initiates an HTTP request for a resource on the Web server, HTTP.sys intercepts the request. HTTP.sys contacts WAS to obtain information from the configuration store. WAS requests configuration information from the configuration store, applicationHost.config. The WWW Service receives configuration information, such as application pool and site configuration. The WWW Service uses the configuration information to configure HTTP.sys. WAS starts a worker process for the application pool to which the request was made. The worker process processes the request and returns a response to HTTP.sys. The client receives a response.","title":"HttpRequest Lifecycle"},{"location":"winforms/","text":"Decoupling Winforms WinForm.cs Button Property: ActionHandler<ArgumentClass> NavigateToOtherWindow Method OnButton.click event { NavigateToOtherWindow(new ArgumentClass(){ Useful info } } The event trigger of the button is coupled to an actionhandler with an interface, but the actionhandler has no implementation. The view only knows when to call the handler and with what info to call it, but it does not know the implementation or provides no implementation. Controller.cs Method Navigate(ArgumentClass args){} --> implementation of logic by controller WinForm.NavigateToOtherWindow = Navigate The controller will provide the implementation and will add this implementation to the actionhandler of the view. This way, the implementation logic is fully decoupled from the view and can be adjusted if necessary.","title":"Winforms"},{"location":"winforms/#decoupling-winforms","text":"","title":"Decoupling Winforms"},{"location":"winforms/#winformcs","text":"Button Property: ActionHandler<ArgumentClass> NavigateToOtherWindow Method OnButton.click event { NavigateToOtherWindow(new ArgumentClass(){ Useful info } } The event trigger of the button is coupled to an actionhandler with an interface, but the actionhandler has no implementation. The view only knows when to call the handler and with what info to call it, but it does not know the implementation or provides no implementation.","title":"WinForm.cs"},{"location":"winforms/#controllercs","text":"Method Navigate(ArgumentClass args){} --> implementation of logic by controller WinForm.NavigateToOtherWindow = Navigate The controller will provide the implementation and will add this implementation to the actionhandler of the view. This way, the implementation logic is fully decoupled from the view and can be adjusted if necessary.","title":"Controller.cs"}]}